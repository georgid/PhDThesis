% This file was created with JabRef 2.10.
% Encoding: MacRoman


@PhdThesis{senturk2016thesis,
  Title                    = {Computational Analysis of Audio Recordings and Music Scores for the Description and Discovery of Ottoman-Turkish Makam Music},
  Author                   = {{\c S}ent{\"u}rk, Sertan},
  School                   = {Universitat Pompeu Fabra},
  Year                     = {2016},

  Address                  = {Barcelona, Spain},
  Month                    = {December}
}

@Conference{senturk2015dunyaMakam_ismir,
  Title                    = {A Tool for The Analysis and Discovery of {Ottoman-{T}urkish} Makam Music},
  Author                   = {Sertan {\c S}ent{\"u}rk and Ferraro, Andr{\'e}s and Alastair Porter and Serra, Xavier},
  Booktitle                = {{Extended Abstracts for the Late Breaking Demo Session of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015)}},
  Year                     = {2015},

  Address                  = {M{\'a}laga, Spain},

  Abstract                 = {We present a web application for the analysis and discovery of Ottoman-{T}urkish makam music. The tool uses an audio-score alignment methodology developed for this music culture for the analysis. It stores the data to be analysed and also executes the analysis algorithms. In the visual interface, the extracted features and alignment results are shown in an audio player, rendering music score and audio representation synchronous to the playback. We plan to improve this prototype in the future mainly for musicological research and music education.},
  Owner                    = {joro},
  Timestamp                = {2017.05.17}
}


@article{dzhambazov2017metrical,
  title={Metrical-accent Aware Vocal Onset Detection in Polyphonic Audio},
  author={Dzhambazov, Georgi and Holzapfel, Andre and Srinivasamurthy, Ajay and Serra, Xavier},
  journal={arXiv preprint arXiv:1707.06163},
  year={2017},
  URL={http://mtg.upf.edu/node/3805}
}


@Article{senturk2014linking_jnmr,
  Title                    = {Linking scores and audio recordings in makam music of {T}urkey},
  Author                   = {{\c{S}}ent{\"{u}}rk, Sertan and Holzapfel, Andre and Serra, Xavier},
  Journal                  = {Journal of New Music Research},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {34--52},
  Volume                   = {43},

  Abstract                 = {The most relevant representations of music are notations and audio recordings, each of which emphasizes a particular perspective and promotes different approximations in the analysis and understanding of music. Linking these two representations and analyzing them jointly should help to better study many musical facets by being able to combine complementary analysis methodologies. In order to develop accurate linking methods, we have to take into account the specificities of a given type of music. In this paper, we present a method for linking musically relevant sections in a score of a piece from makam music of Turkey (MMT) to the corresponding time intervals of an audio recording of the same piece. The method starts by extracting relevant features from the score and from the audio recording. The features of a given score section are compared with the features of the audio recording to find the candidate links in the audio for that score section. Next, using the sequential section information stored in the score, it selects the most likely links. The method is tested on a dataset consisting of instrumental and vocal compositions of MMT, achieving 92.1\% and 96.9\% F1-scores on the instrumental and vocal pieces, respectively. Our results show the importance of culture-specific and knowledge-based approaches in music information processing.},
  File                     = {:publications/senturk2014linking_jnmr.pdf:PDF}
}

@InProceedings{senturk2012approach,
  Title                    = {An Approach for Linking Score and Audio Recordings in Makam Music
 of Turkey},
  Author                   = {\c{S}ent{\"u}rk, Sertan and Holzapfel, Andre and Serra,
 Xavier},
  Booktitle                = {Serra X, Rao P, Murthy H, Bozkurt B, editors. Proceedings of the
 2nd CompMusic Workshop; 2012 Jul 12-13; Istanbul, Turkey. Barcelona:
 Universitat Pompeu Fabra; 2012. p. 95-106.},
  Year                     = {2012},
  Organization             = {Universitat Pompeu Fabra},

  Review                   = {different tunings and extensive usage of non-notated expressive elements.
 
 Addresses the peculiarities in the musical properties of makams in
 conrast to western music
 
 e.g. : non-notated expressive elements (e.g. embelishments), change
 in tuning
 
 
 Starting and ending of sections are provided from the musical score.
 
 
 A synthetic pitch contour is synthesized from the notes, to which
 marks for the section boundaries are linked.
 
 Then the synthetic pitch contour is matched to a f0-contour extracted
 from the audio signal. The result of the matching are time locations
 in the audio, which correspond to the section boundaries.
 
 
 Methodology in detail: 
 
 From the audio recording, the fundamental frequency, f0,
 
 is estimated and processed to obtain an audio pitch contour. The f0
 estimation is also used to calculate a pitch
 
 histogram in order to identify the tuning and the note intervals (Section
 3.2.1). From the score information, we read
 
 the note symbols, the sections and the makam of the piece,
 
 and generate a synthetic pitch contour (Section 3.2.2). In
 
 order to estimate the candidate locations of the sections in
 
 the audio, the method compares these relevant pitch representations
 (Section 3.3). In the Ãnal step, the candidates
 
 are hierarchically checked to link the sections of the score
 
 to the corresponding parts in the audio (Section 3.4).}
}

@InProceedings{anguera2014audio,
  Title                    = {Audio-to-text alignment for speech recognition with very limited resources.},
  Author                   = {Anguera, Xavier and Luque, Jordi and Gracia, Ciro},
  Booktitle                = {INTERSPEECH},
  Year                     = {2014},
  Pages                    = {1405--1409}
}

@InProceedings{atli2014audio,
  Title                    = {Audio feature extraction for exploring {T}urkish makam music},
  Author                   = {Atl{\i}, Hasan Sercan and Uyar, Burak and Sertan {\c S}ent{\"u}rk and Bozkurt, Bar{\i}{\c s} and Serra, Xavier},
  Booktitle                = {{Proceedings of 3rd International Conference on Audio Technologies for Music and Media (ATMM 2014)}},
  Year                     = {2014},

  Address                  = {Ankara, Turkey},
  Pages                    = {142-153},

  File                     = {:publications/atli2014makamFeature_atmm.pdf:PDF}
}

@InProceedings{babacan2013comparative,
  Title                    = {A comparative study of pitch extraction algorithms on a large variety of singing sounds},
  Author                   = {Babacan, Onur and Drugman, Thomas and d'Alessandro, Nicolas and Henrich, Nathalie and Dutoit, Thierry},
  Booktitle                = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {7815--7819}
}

@Book{barber:11:bayesian,
  Title                    = {Bayesian time series models},
  Author                   = {Barber, David and Cemgil, Ali Taylan and Chiappa, Silvia},
  Publisher                = {Cambridge University Press},
  Year                     = {2011},

  ISBN                     = {9780521196765}
}

@Article{benetos2013automatic,
  Title                    = {Automatic music transcription: challenges and future directions},
  Author                   = {Benetos, Emmanouil and Dixon, Simon and Giannoulis, Dimitrios and Kirchhoff, Holger and Klapuri, Anssi},
  Journal                  = {Journal of Intelligent Information Systems},
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {407--434},
  Volume                   = {41},

  Publisher                = {Springer}
}

@InProceedings{bittnermelody,
  Title                    = {Melody extraction by contour classification},
  Author                   = {Bittner, Rachel M and Salamon, Justin and Essid, Slim and Bello, Juan P},
  Booktitle                = {Proc. ISMIR},
  Pages                    = {500--506}
}

@Conference{Bosch,
  Title                    = {A Comparison of Melody Extraction Methods Based on Source-Filter Modelling},
  Author                   = {Bosch, J. and Bittner, R. M. and Salamon, J. and G{\'o}mez, E.},
  Booktitle                = {17th International Society for Music Information Retrieval Conference (ISMIR 2016)},
  Year                     = {2016},

  Address                  = {New York},
  Month                    = {Aug.}
}

@Article{bozkurt2014makamReview,
  Title                    = {Computational Analysis of {T}urkish Makam Music: Review of State-of-the-Art and Challenges},
  Author                   = {Boz\-kurt, Bar{\i}\c{s} and Ayangil, Ruhi and Holzapfel, Andre},
  Journal                  = {Journal of New Music Research},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {3-23},
  Volume                   = {43},

  Doi                      = {10.1080/09298215.2013.865760},
  Owner                    = {joro},
  Timestamp                = {2017.05.09}
}

@InProceedings{repetto2015comparison,
  Title                    = {Comparison of the Singing Style of Two Jingju Schools},
  Author                   = {Caro Repetto, Rafael and Gong, Rong and Kroher, Nadine and Serra, Xavier},
  Booktitle                = {Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015)},
  Year                     = {2015},
  Pages                    = {507--513}
}

@InProceedings{repetto2014creating,
  Title                    = {Creating a Corpus of Jingju (Beijing Opera) Music and Possibilities for Melodic Analysis},
  Author                   = {Caro Repetto, Rafael and Serra, Xavier},
  Booktitle                = {Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014)},
  Year                     = {2014},
  Pages                    = {313--318}
}

@InProceedings{chan2015vocal,
  Title                    = {Vocal activity informed singing voice separation with the iKala dataset},
  Author                   = {Chan, Tak-Shing and Yeh, Tzu-Chun and Fan, Zhe-Cheng and Chen, Hung-Wei and Su, Li and Yang, Yi-Hsuan and Jang, Roger},
  Booktitle                = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  Year                     = {2015},
  Organization             = {IEEE},
  Pages                    = {718--722}
}

@InProceedings{chandna2017monoaural,
  Title                    = {Monoaural audio source separation using deep convolutional neural networks},
  Author                   = {Chandna, Pritish and Miron, Marius and Janer, Jordi and G{\'o}mez, Emilia},
  Booktitle                = {International Conference on Latent Variable Analysis and Signal Separation},
  Year                     = {2017},
  Organization             = {Springer},
  Pages                    = {258--266}
}

@Article{chang2017lyrics,
  Title                    = {Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive Patterns in Vowel Acoustics},
  Author                   = {Chang, Sungkyun and Lee, Kyogu},
  Journal                  = {arXiv preprint arXiv:1701.06078},
  Year                     = {2017}
}

@InProceedings{chen2012chord,
  Title                    = {Chord Recognition Using Duration-explicit Hidden Markov Models.},
  Author                   = {Chen, Ruofeng and Shen, Weibin and Srinivasamurthy, Ajay and Chordia, Parag},
  Booktitle                = {Proceedings of the 13th International Society for Music Information Retrieval Conference},
  Year                     = {2012},
  Pages                    = {445--450},

  Review                   = {duraiotn ditribution p(d)
- has same max Dur D=20 beats for all chords.
- approximated by counts of occurence ( for each chord )}
}

@Misc{Cooke,
  Title                    = {Heterophony},

  Author                   = {Cooke, Peter},
  HowPublished             = {Grove Music Online. \url{http://www.oxfordmusiconline.com/subscriber/article/grove/music/12945}},
  Year                     = {accessed April 5, 2013},

  Owner                    = {joro},
  Publisher                = {Oxford University Press},
  Timestamp                = {2017.05.09}
}



@Misc{Serra:lecture,
  Title                    = {Harmonic Model},

  Author                   = {Serra, Xavier},
  HowPublished             = {Harmonic Model. \url{https://github.com/MTG/sms-tools/blob/master/lectures/06-Harmonic-model/6T1-Harmonic-model.odp}},
  Year                     = {accessed April 7, 2016},

  Owner                    = {joro},
  Timestamp                = {2017.05.09}
}



@Article{ambrosio:99:bninf,
  Title                    = {{Inference in Bayesian networks}},
  Author                   = {D'Ambrosio, Bruce},
  Journal                  = {{AI magazine}},
  Year                     = {1999},
  Number                   = {2},
  Pages                    = {21--36},
  Volume                   = {20}
}

@InProceedings{dittmar2012towards,
  Title                    = {Towards lyrics spotting in the SyncGlobal project},
  Author                   = {Dittmar, Christian and Mercado, Pedro and Grossmann, Holger and Cano, Estefan{\i}a},
  Booktitle                = {Cognitive Information Processing (CIP), 2012 3rd International Workshop on},
  Year                     = {2012},
  Organization             = {IEEE},
  Pages                    = {1--6}
}

@Book{duanmu2000phonology,
  Title                    = {The Phonology of Standard Chinese},
  Author                   = {Duanmu, San},
  Publisher                = {Oxford University Press},
  Year                     = {2000},
  Series                   = {Clarendon Studies in Criminology},

  ISBN                     = {9780198299875},
  Lccn                     = {00059828}
}

@Article{duda1972use,
  Title                    = {Use of the {H}ough transformation to detect lines and curves in pictures},
  Author                   = {Duda, Richard O and Hart, Peter E},
  Journal                  = {Communications of the ACM},
  Year                     = {1972},
  Number                   = {1},
  Pages                    = {11--15},
  Volume                   = {15},

  Publisher                = {ACM}
}

@Article{durga1978voice,
  Title                    = {VOICE CULTURE-With Special Reference to South Indian Music},
  Author                   = {Durga, SAK},
  Journal                  = {Journal of the Indian Musicological Society},
  Year                     = {1978},
  Number                   = {1},
  Pages                    = {5},
  Volume                   = {9},

  Publisher                = {Indian Musicological Society.}
}

@InProceedings{durrieu2009main,
  Title                    = {Main instrument separation from stereophonic audio signals using
 a source/filter model},
  Author                   = {Durrieu, Jean-Louis and Ozerov, Alexey and F{\'e}votte, C{\'e}dric
 and Richard, Ga{\"e}l and David, Bertrand},
  Booktitle                = {European Signal Processing Conference (EUSIPCO), Glasgow, Scotland},
  Year                     = {2009}
}

@InProceedings{dzhambazov2014lyrics_fma,
  Title                    = {Automatic lyrics-to-audio alignment in classical {T}urkish music},
  Author                   = {Dzhambazov, Georgi and {\c{S}}ent{\"{u}}rk, Sertan and Serra, Xavier},
  Booktitle                = {{Proceedings of 4th International Workshop on Folk Music Analysis (FMA 2014)}},
  Year                     = {2014},

  Address                  = {Istanbul, Turkey},
  Pages                    = {61--64},

  Abstract                 = {We apply a lyrics-to-audio alignment state-of-the-art approach to polyphonic pieces from classical Turkish repertoire. A phonetic recognizer is employed, whereby each phoneme is assigned a hidden Markov model (HMM). Initially trained on speech, the models are adapted on singing voice to match the acoustic characteristics of the test dataset. Being the first study on lyrics-to-audio alignment applied on Turkish music, it could serve as a baseline for singing material with similar musical characteristics. As part of this work a dataset of recordings from the classical music tradition is compiled. Experiments, conducted separately for male and female singers, show that female singing is aligned more accurately.},
  File                     = {:publications/dzhambazov2014lyrics_fma.pdf:PDF},
  Keywords                 = {Turkish,Viterbi,alignment,hmm,lyrics,phonemes,singing voice},
  Owner                    = {joro},
  Timestamp                = {2017.04.27}
}

@Conference{dzhambazov_svs_mirex,
  Title                    = {Singing Voice Separation by Harmonic Modeling},
  Author                   = {Dzhambazov,Georgi and Serra, Xavier},
  Booktitle                = {{Proceedings of Music Information Retrieval Evaluation eXchange (MIREX)}},
  Year                     = {2016},

  Address                  = {New York, NY, USA},

  Owner                    = {joro},
  Timestamp                = {2017.04.27}
}

@Conference{dzhambazov2015lyrics_smc,
  Title                    = {Modeling of Phoneme Durations for Alignment between Polyphonic Audio and Lyrics},
  Author                   = {Dzhambazov, Georgi and Serra, Xavier},
  Booktitle                = {{Proceedings of Sound and Music Computing Conference 2015 (SMC 2015)}},
  Year                     = {2015},

  Address                  = {Maynooth, Ireland},

  Abstract                 = {In this work we propose how to modify a standard scheme for text-to-speech alignment for the alignment of lyrics and singing voice. To this end we model the duration of phonemes specific for the case of singing. We rely on a duration-explicit hidden Markov model (DHMM) phonetic recognizer based on mel frequency cepstral coefficients (MFCCs), which are extracted in a way robust to background instrumental sounds. The proposed approach is tested on polyphonic audio from the classical Turkish music tradition in two settings: with and without modeling phoneme durations. Phoneme durations are inferred from sheet music. In order to assess the impact of the polyphonic setting, alignment is evaluated as well on an acapella dataset, compiled especially for this study. We show that the explicit modeling of phoneme durations improves alignment accuracy by absolute 10 percent on the level of lyrics lines (phrases) and performs on par with state-of-the-art aligners for other languages.},
  Owner                    = {joro},
  Timestamp                = {2017.04.27}
}

@InProceedings{dzhambazov2016onsetLyrics_ismir,
  Title                    = {On the use of note onsets for improved lyrics-to-audio alignment in {T}urkish makam music},
  Author                   = {Dzhambazov, Georgi and Srinivasamurthy, Ajay and {\c S}en\-t{\"u}rk, Sertan and Serra, Xavier},
  Booktitle                = {{Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR 2016)}},
  Year                     = {2016},

  Address                  = {New York, NY, USA},
  Pages                    = {716--722},

  Owner                    = {joro},
  Timestamp                = {2017.04.27}
}

@Conference{dzhambazov_lyrics_jingju,
  Title                    = {Automatic Alignment of Long Syllables In a cappella {B}eijing Opera},
  Author                   = {Dzhambazov, Georgi and Yang, Yile and Repetto, Rafael Caro and Serra, Xavier},
  Booktitle                = {Proceedings of 6th International Workshop on Folk Music Analysis (FMA 2016)},
  Year                     = {2016},

  Address                  = {Dublin, Ireland},
  Pages                    = {88--91},

  Abstract                 = {In this study we propose how to modify a standard approach for text-to-speech alignment to apply in the case of alignment of lyrics and singing voice. We model phoneme durations by means of a duration-explicit hidden Markov model (DHMM) phonetic recognizer based on MFCCs. The phoneme durations are empirically set in a probabilistic way, based on prior knowledge about the lyrics structure and metric principles, specific for the Beijing opera music tradition. Phoneme models are GMMs trained directly on a small corpus of annotated singing voice. The alignment is evaluated on a cappella material from Beijing opera, which is characterized by its particularly long syllable durations. Results show that the incorporation of music-specific knowledge results in a very high alignment accuracy, outperforming significantly a baseline HMM-based approach.},
  Owner                    = {joro},
  Timestamp                = {2017.04.27}
}

@Book{ederer2011theory,
  Title                    = {The Theory and Praxis of Makam in Classical Turkish Music 1910--2010},
  Author                   = {Ederer, Eric Bernard},
  Publisher                = {University of California, Santa Barbara},
  Year                     = {2011}
}

@InProceedings{ferguson1980variable,
  Title                    = {Variable duration models for speech},
  Author                   = {Ferguson, Jack D},
  Booktitle                = {Symposium on the Application of Hidden Markov Models to Text and Speech, 1980},
  Year                     = {1980},
  Pages                    = {143--179}
}

@InProceedings{frostel2011vowel,
  Title                    = {The Vowel Worm: Real-time Mapping and Visualisation of Sung Vowels in Music},
  Author                   = {Frostel, Harald and Arzt, Andreas and Widmer, Gerhard},
  Booktitle                = {Proceedings of the 8th Sound and Music Computing Conference},
  Year                     = {2011},
  Pages                    = {214--219}
}

@Article{fujihara2012lyrics,
  Title                    = {Lyrics-to-audio alignment and its application},
  Author                   = {Fujihara, Hiromasa and Goto, Masataka},
  Journal                  = {Dagstuhl Follow-Ups},
  Year                     = {2012},
  Volume                   = {3},

  Publisher                = {Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik},
  Review                   = {It is known that the singing voice has more
complicated frequency and dynamic characteristics than speech [20]. For example,
fluctuation of fundamental frequency (F0) 2 and loudness of singing voices are far stronger
than those of speech sounds.}
}

@INPROCEEDINGS{Goto02rwcmusic,
    author = {Masataka Goto and Hiroki Hashiguchi and Takuichi Nishimura and Ryuichi Oka},
    title = {R{W}{C} Music Database: Popular, Classical, and Jazz Music Databases},
    booktitle = {Proceedings of the 3rd International Conference on Music Information Retrieval (ISMIR 2002)},
    year = {2002},
    pages = {287--288}
}

@inproceedings{holzapfel2016bayesian,
  title={Bayesian meter tracking on learned signal representations},
  author={Holzapfel, Andre and Grill, Thomas},
  booktitle={Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR 2016)},
  pages={262--268},
  year={2016}
}

@inproceedings{chang2014pairwise,
  title={A pairwise approach to simultaneous onset/offset detection for singing voice using correntropy},
  author={Chang, Sungkyun and Lee, Kyogu},
  booktitle={Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={629--633},
  year={2014},
  organization={IEEE}
}

@inproceedings{degara2010note,
  title={Note onset detection using rhythmic structure},
  author={Degara, Norberto and Pena, Antonio and Davies, Matthew EP and Plumbley, Mark D},
  booktitle={Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5526--5529},
  year={2010},
  organization={IEEE}
}

@InProceedings{fujihara2008hyperlinking,
  Title                    = {Hyperlinking Lyrics: A Method for Creating Hyperlinks Between Phrases in Song Lyrics},
  Author                   = {Fujihara, Hiromasa and Goto, Masataka and Ogata, Jun},
  Booktitle                = {Proceedings of the 9th International Conference on Music Information Retrieval},
  Year                     = {2008},

  Address                  = {Philadelphia, USA},
  Month                    = {September 14-18},
  Pages                    = {281--286}
}

@Article{fujihara2011lyricsynchronizer,
  Title                    = {LyricSynchronizer: Automatic synchronization system between musical audio signals and lyrics},
  Author                   = {Fujihara, Hiromasa and Goto, Masataka and Ogata, Jun and Okuno, Hiroshi G},
  Journal                  = {IEEE Journal of Selected Topics in Signal Processing},
  Year                     = {2011},
  Number                   = {6},
  Pages                    = {1252--1261},
  Volume                   = {5},

  Publisher                = {IEEE},
  Review                   = {1. vocal segregation: 
- find f-0 contour of most predominant melody. extract harmonic structure crresponding to melody. resynthesize melodic line
how does the newly sznthesized sound keeps the formants for the vowels. Are they based on the harmonics ? How does it sound?

2. run voice/non-voice HMM detector
- based on a novel feature

3. detect fricative sounds in the original sygnal (s, z, f, )-
- because and as non-voiced have no harmonic structure and thus are not detected by the vocal segr. module. 
fricatives are longer
Then at the candidate timestamps of fricatives on the alignment a fricative is imposed.


Section II.D - Viterbi alignment: 

only the vocals are aligned

 Adaptation is based on: 
MLLR, MAP but how: which for which phonemes?}
}

@InProceedings{fujihara2009novel,
  Title                    = {A novel framework for recognizing phonemes of singing voice in polyphonic music},
  Author                   = {Fujihara, Hiromasa and Goto, Masataka and Okuno, Hiroshi G},
  Booktitle                = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA'09) },
  Year                     = {2009},
  Organization             = {IEEE},
  Pages                    = {17--20}
}

@Article{gomez2013towards,
  Title                    = {Towards computer-assisted flamenco transcription: An experimental comparison of automatic transcription algorithms as applied to a cappella singing},
  Author                   = {G{\'o}mez, Emilia and Bonada, Jordi},
  Journal                  = {Computer Music Journal},
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {73--90},
  Volume                   = {37},

  Publisher                = {MIT Press}
}

@InProceedings{gong2015real,
  Title                    = {Real-Time Audio-to-Score Alignment of Singing Voice Based on Melody and Lyric Information},
  Author                   = {Rong Gong and Philippe Cuvillier and Nicolas Obin and Arshia Cont},
  Booktitle                = {Interspeech 2015},
  Year                     = {2015},

  Address                  = {Dresden, Germany},
  Month                    = {06/09/2015},

  Abstract                 = {Singing voice is specific in music: a vocal performance con- veys both music (melody/pitch) and lyrics (text/phoneme) con- tent. This paper aims at exploiting the advantages of melody and lyric information for real-time audio-to-score alignment of singing voice. First, lyrics are added as a separate observa- tion stream into a template-based hidden semi-Markov model (HSMM), whose observation model is based on the construc- tion of vowel templates. Second, early and late fusion of melody and lyric information are processed during real-time audio-to-score alignment. An experiment conducted with two professional singers (male/female) shows that the performance of a lyrics-based system is comparable to that of melody-based score following systems. Furthermore, late fusion of melody and lyric information substantially improves the alignment per- formance. Finally, maximum a posteriori adaptation (MAP) of the vowel templates from one singer to the other suggests that lyric information can be efficiently used for any singer.}
}

@Conference{gong:hal-01513160,
  Title                    = {Score-Informed Syllable Segmentation for Jingju a Cappella Singing Voice with Mel-Frequency Intensity Profiles},
  Author                   = {Rong Gong and Nicolas Obin and Georgi Dzhambazov and Xavier Serra},
  Booktitle                = {Proceedings of 7th International Workshop on Folk Music Analysis (FMA 2017)},
  Year                     = {2017},

  Address                  = {Malaga, Spain},
  Month                    = {14/06/2017},

  Doi                      = {https://doi.org/10.5281/zenodo.556820},
  Owner                    = {joro},
  Timestamp                = {2017.05.15},
  Url                      = {http://mtg.upf.edu/node/3732}
}

@InProceedings{goto2014singing,
  Title                    = {Singing information processing},
  Author                   = {Goto, Masataka},
  Booktitle                = {12th International Conference on Signal Processing (ICSP)},
  Year                     = {2014},
  Organization             = {IEEE},
  Pages                    = {2431--2438}
}

@PhdThesis{Gulati,
  Title                    = {Computational Approaches for Melodic Description in Indian Art Music Corpora},
  Author                   = {Sankalp Gulati},
  School                   = {Universitat Pompeu Fabra},
  Year                     = {2016},

  Address                  = {Barcelona},

  Abstract                 = {<p class="rtejustify">Automatically describing contents of recorded music is crucial for interacting with large volumes of audio recordings, and for developing novel tools to facilitate music pedagogy. Melody is a fundamental facet in most music traditions and, therefore, is an indispensable component in such description. In this thesis, we develop computational approaches for analyzing high-level melodic aspects of music performances in Indian art music (IAM), with which we can describe and interlink large amounts of audio recordings. With its complex melodic framework and well-grounded theory, the description of IAM melody beyond pitch contours offers a very interesting and challenging research topic. We analyze melodies within their tonal context, identify<br /> melodic patterns, compare them both within and across music pieces, and finally, characterize the specific melodic context of IAM, the r{\={a}}gas. All these analyses are done using data-driven methodologies on sizable curated music corpora. Our work paves the way for addressing several interesting research problems in the field of music information research, as well as developing novel applications in the context of music discovery and music pedagogy.</p>
<p class="rtejustify">The thesis starts by compiling and structuring largest to date music corpora of the two IAM traditions, Hindustani and Carnatic music, comprising quality audio recordings and the associated metadata. From them we extract the predominant pitch and normalize by the tonic context. An important element to describe melodies is the identification of the meaningful temporal units, for which we propose to detect occurrences of ny{\={a}}s svaras in Hindustani music, a landmark that demarcates musically salient melodic patterns.</p>
<p class="rtejustify">Utilizing these melodic features, we extract musically relevant recurring melodic patterns. These patterns are the building blocks of melodic structures in both improvisation and composition. Thus, they are fundamental to the description of audio collections in IAM. We propose an unsupervised approach that employs time-series analysis tools to discover melodic patterns in sizable music collections. We first carry out an in-depth supervised analysis of melodic similarity, which is a critical component in pattern discovery. We then improve upon the best possible competing approach by<br /> exploiting peculiar melodic characteristics in IAM. To identify musically meaningful patterns, we exploit the relationships between the discovered patterns by performing a network analysis. Extensive listening tests by professional musicians reveal that the discovered melodic patterns are musically interesting and significant.</p>
<p class="rtejustify">Finally, we utilize our results for recognizing r{\={a}}gas in recorded performances of IAM. We propose two novel approaches that jointly capture the tonal and the temporal aspects of melody. Our first approach uses melodic patterns, the most prominent cues for r{\={a}}ga identification by humans. We utilize the discovered melodic patterns and employ topic modeling techniques, wherein we regard a r{\={a}}ga rendition similar to a textual description of a topic. In our second approach, we propose the time delayed melodic surface, a novel feature based on delay coordinates that captures the melodic outline of a r{\={a}}ga. With these approaches we demonstrate unprecedented accuracies in r{\={a}}ga recognition on the largest datasets ever used for this task. Although our approach is guided by the characteristics of melodies in IAM and the task at hand, we believe our methodology can be easily extended to other melody dominant music traditions.</p>
<p class="rtejustify">Overall, we have built novel computational methods for analyzing several melodic aspects of recorded performances in IAM, with which we describe and interlink large amounts of music recordings. In this process we have developed several tools and compiled data that can be used for a number of computational studies in IAM, specifically in characterization of r{\={a}}gas, compositions and artists. The technologies resulted from this research work are a part of several applications developed within the CompMusic project for a better description, enhanced listening experience, and pedagogy in IAM.</p>
},
  Keywords                 = {audio analysis, carnatic, chalan, CompMusic, computational analysis, dtw, dynamic time warping, hindustani, Indian art music, melodic description, melodic patterns, Melody, motifs, music information retrieval, Nyas, pakad, pattern network, patterns, phrases, raaga, raga, raga recognition, svara, time delayed melodic surface, Time series, Tonic identification, vector space modeling},
  Pages                    = {305}
}

@InProceedings{hansen2012recognition,
  Title                    = {Recognition of Phonemes in A-cappella Recordings using Temporal Patterns and Mel Frequency Cepstral Coefficients},
  Author                   = {Hansen, Jens Kofod},
  Booktitle                = {Proceedings of the 9th Sound and Music Computing Conference},
  Year                     = {2012},

  Address                  = {Copenhagen, Denmark},
  Pages                    = {494--499}
}

@Article{holzapfel2015relation,
  Title                    = {Relation between surface rhythm and rhythmic modes in Turkish makam music},
  Author                   = {Holzapfel, Andre},
  Journal                  = {Journal of New Music Research},
  Year                     = {2015},
  Number                   = {1},
  Pages                    = {25--38},
  Volume                   = {44},

  Publisher                = {Taylor \& Francis}
}

@Conference{holzapfelsection,
  Title                    = {Section-level Modeling of Musical Audio for Linking Performances to Scores in {Turkish} Makam Music},
  Author                   = {Holzapfel, Andre and {\c S}im{\c s}ekli, Umut and Sertan {\c S}ent{\"u}rk and Cemgil, Ali Taylan},
  Booktitle                = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  Year                     = {2015},

  Address                  = {Brisbane, Australia},
  Month                    = {19/04/2015},

  Abstract                 = {Section linking aims at relating structural units in the notation of a piece of music to their occurrences in a performance of the piece. In this paper, we address this task by presenting a score-informed hierarchical Hidden Markov Model (HHMM) for modeling musical audio signals on the temporal level of sections present in a composition, where the main idea is to explicitly model the long range and hierarchical structure of music signals. So far, approaches based on HHMM or similar methods were mainly developed for a note-to-note alignment, i.e. an alignment based on shorter temporal units than sections. Such approaches, however, are conceptually problematic when the performances differ substantially from the reference score due to interpretation and improvisation, a very common phenomenon, for instance, in {Turkish} makam music. In addition to having low computational complexity compared to note-to-note alignment and achieving a transparent and elegant model, the experimental results show that our method outperforms a previously presented approach on a {Turkish} makam music corpus.},
  Url                      = {http://sertansenturk.com/uploads/publications/holzapfel2015linking_icassp.pdf}
}

@InProceedings{holzapfel2014tracking,
  Title                    = {{Tracking the ``odd'': Meter inference in a culturally diverse music corpus}},
  Author                   = {Holzapfel, Andre and Krebs, Florian and Srinivasamurthy, Ajay},
  Booktitle                = {{Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014)}},
  Year                     = {2014},

  Address                  = {Taipei, Taiwan},
  Pages                    = {425--430},

  Owner                    = {hannover},
  Timestamp                = {2014.07.18}
}

@Book{huron2006sweet,
  Title                    = {Sweet anticipation: Music and the psychology of expectation},
  Author                   = {Huron, David Brian},
  Publisher                = {MIT press},
  Year                     = {2006},

  Owner                    = {joro},
  Timestamp                = {2017.04.28}
}

@Article{ishwar2014pitch,
  Title                    = {Pitch Estimation of the Predominant Vocal Melody from Heterophonic Music Audio Recordings},
  Author                   = {Ishwar, Vignesh},
  Year                     = {2014}
}

@InProceedings{janer2013separation,
  Title                    = {Separation of unvoiced fricatives in singing voice mixtures with semi-supervised NMF},
  Author                   = {Janer, Jordi and Marxer, Ricard},
  Booktitle                = {Proc. 16th International Conference on Digital Audio Effects (DAFx)},
  Year                     = {2013}
}

@Article{johnson2005capacity,
  Title                    = {Capacity and complexity of {HMM} duration modeling techniques},
  Author                   = {Johnson, Michael T},
  Journal                  = {Signal Processing Letters, IEEE},
  Year                     = {2005},
  Number                   = {5},
  Pages                    = {407--410},
  Volume                   = {12},

  Publisher                = {IEEE}
}

@InProceedings{karaosmanouglu2012turkish,
  Title                    = {A {T}urkish makam music symbolic database for music information retrieval: SymbTr},
  Author                   = {Karaosmano{\u{g}}lu, M Kemal},
  Booktitle                = {Proceedings of the 13th International Society for Music Information Retrieval Conference},
  Year                     = {2012},

  Address                  = {Porto, Portugal},

  Owner                    = {joro},
  Timestamp                = {2017.04.19}
}

@InProceedings{karaosmanouglu2014symbolic,
  Title                    = {A Symbolic Dataset of {T}urkish Makam Music Phrases},
  Author                   = {Karaosmano{\u{g}}lu, M Kemal and Bozkurt, Bar{\i}{\c{s}} and Holzapfel, Andre and Do{\u{g}}rus{\"o}z Di{\c{s}}ia{\c{c}}{\i}k, Nilg{\"u}n},
  Booktitle                = {Fourth International Workshop on Folk Music Analysis (FMA2014)},
  Year                     = {2014},

  Owner                    = {joro},
  Timestamp                = {2017.04.19}
}

@Book{klapuri:06:transcription,
  Title                    = {{Signal Processing Methods for Music Transcription}},
  Author                   = {Klapuri, Anssi and Davy, Manuel},
  Publisher                = {{Springer}},
  Year                     = {2006},

  Owner                    = {joro},
  Timestamp                = {2014.04.09.18}
}

@Book{koller:09:pgm,
  Title                    = {Probabilistic Graphical Models: Principles and Techniques},
  Author                   = {Koller,Daphne and Friedman, Nir},
  Publisher                = {MIT Press},
  Year                     = {2009},

  Owner                    = {Ajay},
  Timestamp                = {2016.06.18}
}

@MastersThesis{FilipsThesis,
  Title                    = {Real-time Capable Singer Tracking Using Pitch and Lyrics Information},
  Author                   = {Filip Korzeniowski},
  Year                     = {2011},

  Review                   = {The phonetic features are : 
 
 a vector of the 9 log-probabilities of the 9 vocals in the vocal trapezoid
 
 for a given vocal a model with these 9 probabilitiesis trained using
 the vocal joystick voiwe corpus.
 
 
 It has been proposed a HMM that has 2 dimensions - horizaontal note
 states dimension
 
 and 
 
 vertical - phoneme state duration
 
 HOWEVER! this model is not applied but a simple mmoel : 
 
 The vowel information is incorporated in the output distributions
 using only a single stationary manifestation per note
 
 
 ------------------
 
 the vowel and the pitch features are mutually independent. ... This
 justifies the fact that we can consider them as two independent sets
 of features
 
 
 According to the method found in Rap99 the output probability of a
 combined feature vector can be represented as the product of output
 probability of the two independent feature sets. 
 
 Since a feature set is driven by an indep. Gaussian, we can combine
 them straightforward into a common Gaussian.},
  Timestamp                = {2013.05.07}
}

@InProceedings{krebs:15:ismir,
  Title                    = {{An Efficient State-Space Model for Joint Tempo and Meter Tracking}},
  Author                   = {Krebs, Florian and B{\"o}ck, Sebastian and Widmer, Gerhard},
  Booktitle                = {{Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015)}},
  Year                     = {2015},

  Address                  = {Malaga, Spain},
  Month                    = oct,
  Pages                    = {72--78},

  Owner                    = {Ajay},
  Timestamp                = {2016.01.04}
}

@InProceedings{krebs2013rhythmic,
  Title                    = {Rhythmic pattern modeling for beat and downbeat tracking in musical audio},
  Author                   = {Krebs, Florian and B{\"o}ck, Sebastian and Widmer, Gerhard},
  Booktitle                = {Proceedings of the 14th International Society for Music Information Retrieval Conference (ISMIR 2013)},
  Year                     = {2013},

  Address                  = {Curitiba, Brazil}
}

@Article{krige2008explicit,
  Title                    = {Explicit transition modelling for automatic singing transcription},
  Author                   = {Krige, Willie and Herbst, Theo and Niesler, Thomas},
  Journal                  = {Journal of New Music Research},
  Year                     = {2008},
  Number                   = {4},
  Pages                    = {311--324},
  Volume                   = {37},

  Publisher                = {Taylor \& Francis}
}

@Article{kroher2015automatic,
  Title                    = {Automatic Transcription of Flamenco Singing from Polyphonic Music Recordings},
  Author                   = {Kroher, Nadine and G{\'o}mez, Emilia},
  Journal                  = {IEEE Transactions on Audio, Speech and Language Processing},
  Year                     = {2016},
  Number                   = {5},
  Pages                    = {901--913},
  Volume                   = {24}
}

@InProceedings{kruspe2016bootstrapping,
  Title                    = {Bootstrapping a system for phoneme recognition and keyword spotting in unaccompanied singing},
  Author                   = {Kruspe, Anna M},
  Booktitle                = {{Proceedings of 17th International Society for Music Information Retrieval Conference (ISMIR 2016)}},
  Year                     = {2016},

  Address                  = {New York, NY, USA}
}

@InProceedings{kruspe2015keyword,
  Title                    = {Keyword spotting in singing with duration-modeled {HMM}s},
  Author                   = {Kruspe, Anna M},
  Booktitle                = {In Proceedings of 23rd European Signal Processing Conference (EUSIPCO)},
  Year                     = {2015},
  Organization             = {IEEE},
  Pages                    = {1291--1295}
}

@InProceedings{kruspe2015training,
  Title                    = {Training phoneme models for singing with "songified" speech data},
  Author                   = {Kruspe, Anna M},
  Booktitle                = {{Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015)}},
  Year                     = {2015}
}

@InProceedings{kruspekeyword,
  Title                    = {Keyword Spotting in A-capella Singing},
  Author                   = {Kruspe, Anna M},
  Booktitle                = {Proceedings of the 15th International Society for Music Information Retrieval Conference},
  Year                     = {2014},

  Address                  = {Taipei, Taiwan},
  Pages                    = {271-276},

  Review                   = {Approach: 
first step : phoneme Recognition by MLP. sperately with 3 different features : MFCC, PLP and TRAP
second step: postprocessing and combining of features
third HMM-based keyword spotting. as input used postetior probs from HMMs

\
Dataset: 19 clean singing vocie recordings

Conslucion: MLP trained on singing voice better than MLP trained on speech}
}

@Book{ladefoged1996elements,
  Title                    = {Elements of acoustic phonetics},
  Author                   = {Ladefoged, Peter},
  Publisher                = {University of Chicago Press},
  Year                     = {1996}
}

@InProceedings{lee2008segmentation,
  Title                    = {Segmentation-Based Lyrics-Audio Alignment using Dynamic Programming.},
  Author                   = {Lee, Kyogu and Cremer, Markus},
  Booktitle                = {{Proceedings of 9th International Society for Music Information Retrieval Conference (ISMIR 2008)}},
  Year                     = {2008},
  Pages                    = {395--400},

  Owner                    = {joro},
  Review                   = {problem: match manually-labeled textual paragraphs to not-semantically meaningful but labeled structured audio segments (same letter for segments in same cluster). 
approach: 
step 1: re-label audio segments (as chorus, verse) to match paragraphal labels using some simple empirical rules . (most repetitive and so on)
step 2: cost matrix (btw different chorus and verse e.g. ) again with empirical rules.
step 3: s.th. like levenstien

No contentual information, no lyrics},
  Timestamp                = {2015.02.20}
}

@InProceedings{lehner2015low,
  Title                    = {A low-latency, real-time-capable singing voice detection method with LSTM recurrent neural networks},
  Author                   = {Lehner, Bernhard and Widmer, Gerhard and Bock, Sebastian},
  Booktitle                = {Signal Processing Conference (EUSIPCO), 2015 23rd European},
  Year                     = {2015},
  Organization             = {IEEE},
  Pages                    = {21--25}
}

@InProceedings{lehner2014reduction,
  Title                    = {On the reduction of false positives in singing voice detection},
  Author                   = {Lehner, Bernhard and Widmer, Gerhard and Sonnleitner, Reinhard},
  Booktitle                = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  Year                     = {2014},
  Organization             = {IEEE},
  Pages                    = {7480--7484}
}

@Article{levy2008structural,
  Title                    = {Structural segmentation of musical audio by constrained clustering},
  Author                   = {Levy, Mark and Sandler, Mark},
  Journal                  = {IEEE Transactions on Audio, Speech, and Language Processing},
  Year                     = {2008},
  Number                   = {2},
  Pages                    = {318--326},
  Volume                   = {16},

  Publisher                = {IEEE}
}

@InProceedings{Loscos99low-delaysinging,
  Title                    = {Low-Delay Singing Voice Alignment to Text},
  Author                   = {Alex Loscos and Pedro Cano and Jordi Bonada},
  Booktitle                = {Proceedings of the ICMC},
  Year                     = {1999}
}

@Book{muller2007information,
  Title                    = {Information retrieval for music and motion},
  Author                   = {M{\"u}ller, Meinard},
  Publisher                = {Springer},
  Year                     = {2007},
  Volume                   = {2}
}

@InCollection{muller2007lyrics,
  Title                    = {Lyrics-based audio retrieval and multimodal navigation in music collections},
  Author                   = {M{\"u}ller, Meinard and Kurth, Frank and Damm, David and Fremerey, Christian and Clausen, Michael},
  Booktitle                = {Research and Advanced Technology for Digital Libraries},
  Publisher                = {Springer},
  Year                     = {2007},
  Pages                    = {112--123}
}

@Article{mandal2014recent,
  Title                    = {Recent developments in spoken term detection: a survey},
  Author                   = {Mandal, Anupam and Kumar, KR Prasanna and Mitra, Pabitra},
  Journal                  = {International Journal of Speech Technology},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {183--198},
  Volume                   = {17},

  Publisher                = {Springer}
}

@Book{Manning:2008:IIR:1394399,
  Title                    = {Introduction to Information Retrieval},
  Author                   = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
  Publisher                = {Cambridge University Press},
  Year                     = {2008},

  Address                  = {New York, NY, USA},

  ISBN                     = {0521865719, 9780521865715}
}

@PhdThesis{mauch:thesis:2010,
  Title                    = {Automatic Chord Transcription from Audio Using Computational Models of Musical Context},
  Author                   = {Matthias Mauch},
  School                   = {Queen Mary University of London},
  Year                     = {2010}
}

@InProceedings{mauch2015computer,
  Title                    = {Computer-aided melody note transcription using the {T}ony software: Accuracy and efficiency},
  Author                   = {Mauch, Matthias and Cannam, Chris and Bittner, Rachel and Fazekas, George and Salamon, Justin and Dai, Jiajie and Bello, Juan and Dixon, Simon},
  Booktitle                = {Proceedings of the First International Conference on Technologies for Music Notation and Representation (TENOR 2015)},
  Year                     = {2015},
  Pages                    = {23--30}
}


@Article{mauch2012integrating,
  Title                    = {Integrating additional chord information into {HMM}-based lyrics-to-audio alignment},
  Author                   = {Mauch, Matthias and Fujihara, Hiromasa and Goto, Masataka},
  Journal                  = {IEEE Transactions on Audio, Speech, and Language Processing},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {200--210},
  Volume                   = {20},

  Publisher                = {IEEE}
}

@Article{mcnab1995signal,
  Title                    = {Signal processing for melody transcription},
  Author                   = {McNab, Rodger J and Smith, Lloyd A and Witten, Ian H},
  Year                     = {1995},

  Publisher                = {University of Waikato, Department of Computer Science},
  Review                   = {several pitch based and amplitude based segmentation methods}
}

@InProceedings{mcvicar2014leveraging,
  Title                    = {Leveraging repetition for improved automatic lyric transcription in popular music},
  Author                   = {McVicar, Matt and Ellis, Daniel PW and Goto, Masataka},
  Booktitle                = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  Year                     = {2014},
  Organization             = {IEEE},
  Pages                    = {3117--3121}
}

@Article{mesaros2012singing,
  Title                    = {Singing voice recognition for music information retrieval},
  Author                   = {Mesaros, Annamaria},
  Journal                  = {Tampereen teknillinen yliopisto. Julkaisu-Tampere University of Technology. Publication; 1064},
  Year                     = {2012}
}

@Article{mesaros2010automatic,
  Title                    = {Automatic recognition of lyrics in singing},
  Author                   = {Mesaros, Annamaria and Virtanen, Tuomas},
  Journal                  = {EURASIP Journal on Audio, Speech, and Music Processing},
  Year                     = {2010},
  Number                   = {1},
  Pages                    = {546047},
  Volume                   = {2010},

  Publisher                = {Springer International Publishing}
}

@InProceedings{mesaros2008automatic,
  Title                    = {Automatic alignment of music audio and lyrics},
  Author                   = {Mesaros, Annamaria and Virtanen, Tuomas},
  Booktitle                = {Proceedings of the 11th International Conference on Digital Audio Effects (DAFx-08)},
  Year                     = {2008},

  Review                   = {viterbi decoding used, but restricted to one string of phonemes.
 
 manual alignment of start of lyrial lines to text made. 
 
 
 there is first vocal line extraction model
 
 
 model: 
 
 3-state phonemes from speech corpus, adapted using MLLR to clean singing
 voice. 
 
 + 
 
 background noise model having different amoount of states. 
 
 
 
 -----
 
 Problems of the lyrics-audio alignment task: 
 
 1) there are significant differences in the dynamics and general properties
 of speech and singing sounds. 
 
 2) The com- plexity of the polyphonic music signal compared to a pure
 singing voice signal}
}

@Article{molina2015sipth,
  Title                    = {Sipth: Singing transcription based on hysteresis defined on the pitch-time curve},
  Author                   = {Molina, Emilio and Tard{\'o}n, Lorenzo J and Barbancho, Ana M and Barbancho, Isabel},
  Journal                  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  Year                     = {2015},
  Number                   = {2},
  Pages                    = {252--263},
  Volume                   = {23},

  Publisher                = {IEEE}
}

@InProceedings{molina2014importance,
  Title                    = {The Importance of F0 Tracking in Query-by-singing-humming.},
  Author                   = {Molina, Emilio and Tard{\'o}n, Lorenzo J and Barbancho, Isabel and Barbancho, Ana M},
  Booktitle                = {Proceedings of the 15th International Society for Music Information Retrieval Conference},
  Year                     = {2014},

  Address                  = {Taipei, Taiwan},
  Pages                    = {277--282}
}

@PhdThesis{murphy2002dynamic,
  Title                    = {Dynamic bayesian networks: representation, inference and learning},
  Author                   = {Murphy, Kevin Patrick},
  School                   = {University of California},
  Year                     = {2002}
}

@InProceedings{nichols2009relationships,
  Title                    = {Relationships Between Lyrics and Melody in Popular Music},
  Author                   = {Nichols, Eric and Morris, Dan and Basu, Sumit and Raphael, Christopher},
  Booktitle                = {{Proceedings of the 10th International Society for Music Information Retrieval Conference (ISMIR 2009)}},
  Year                     = {2009},
  Pages                    = {471--476}
}

@InProceedings{NishikimiNIY16,
  Title                    = {Musical Note Estimation for {F0} Trajectories of Singing Voices Based
 on a Bayesian Semi-Beat-Synchronous {HMM}},
  Author                   = {Ryo Nishikimi and
 Eita Nakamura and
 Katsutoshi Itoyama and
 Kazuyoshi Yoshii},
  Booktitle                = {Proceedings of the 17th International Society for Music Information Retrieval Conference, (ISMIR 2016)},
  Year                     = {2016},
  Pages                    = {461--467},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/conf/ismir/NishikimiNIY16},
  Timestamp                = {Thu, 08 Sep 2016 13:32:51 +0200}
}

@InProceedings{orio2001score,
  Title                    = {Score following using spectral analysis and hidden Markov models},
  Author                   = {Orio, Nicola and D{\'e}chelle, Fran{\c{c}}ois},
  Booktitle                = {ICMC: International Computer Music Conference},
  Year                     = {2001},
  Pages                    = {1--1}
}

@Book{judetz1996meanings,
  Title                    = {Meanings in {T}urkish Musical Culture},
  Author                   = {Popescu-Judetz, Eugenia},
  Publisher                = {Pan Yay{\i}nc{\i}l{\i}k},
  Year                     = {1996},

  Address                  = {Istanbul},

  ISBN                     = {9757652539},
  Owner                    = {joro},
  Timestamp                = {2017.05.09}
}

@Conference{porter2013dunya_ismir,
  Title                    = {Dunya: A System for Browsing Audio Music Collections Exploiting Cultural Context},
  Author                   = {Porter, Alastair and Sordo, Mohamed and Serra, Xavier},
  Booktitle                = {{Proceedings of 14th International Society for Music Information Retrieval Conference (ISMIR 2013)}},
  Year                     = {2013},

  Address                  = {Curitiba, Brazil},
  Pages                    = {101-106},

  Abstract                 = {Music recommendation and discovery is an important MIR application with a strong impact in the music industry, but
most music recommendation systems are still quite generic and without much musical knowledge. In this paper we
present a web-based software application that lets users interact with an audio music collection through the use of
musical concepts that are derived from a specific musical culture, in this case Carnatic music. The application includes a database containing information relevant to that music collection, such as audio recordings, editorial information, and metadata obtained from various sources. An analysis module extracts features from the audio recordings that are related to Carnatic music, which are then used to create musically meaningful relationships between all
of the items in the database. The application displays the content of these items, allowing users to navigate through
the collection by identifying and showing other information that is related to the currently viewed item, either by
showing the relationships between them or by using culturally relevant similarity measures. The basic architecture
and the design principles developed are reusable for other music collections with different characteristics.},
  Owner                    = {joro},
  Timestamp                = {2017.05.17}
}

@InProceedings{roebel2009onset,
  Title                    = {Onset detection by means of transient peak classification in harmonic bands},
  Author                   = {R{\"o}bel, Axel},
  Booktitle                = {Music Information Retrieval Evaluation eXchange (MIREX)},
  Year                     = {2009}
}

@Article{rabiner1989tutorial,
  Title                    = {A tutorial on hidden {Markov} models and selected applications in speech recognition},
  Author                   = {Rabiner, Lawrence},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {1989},
  Number                   = {2},
  Pages                    = {257--286},
  Volume                   = {77},

  Publisher                = {IEEE}
}

@Book{Rabiner:1993:FSR:153687,
  Title                    = {Fundamentals of Speech Recognition},
  Author                   = {Rabiner, Lawrence and Juang, Biing-Hwang},
  Publisher                = {Prentice-Hall, Inc.},
  Year                     = {1993},

  Address                  = {Upper Saddle River, NJ, USA},

  ISBN                     = {0-13-015157-2}
}

@PhdThesis{raffel2016learning,
  Title                    = {Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching},
  Author                   = {Raffel, Colin},
  School                   = {Columbia University},
  Year                     = {2016}
}

@InProceedings{raffel2014mir_eval,
  Title                    = {mir\_eval: A transparent implementation of common MIR metrics},
  Author                   = {Raffel, Colin and McFee, Brian and Humphrey, Eric J and Salamon, Justin and Nieto, Oriol and Liang, Dawen and Ellis, Daniel PW and Raffel, C Colin},
  Booktitle                = {In Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR},
  Year                     = {2014},
  Organization             = {Citeseer}
}

@InProceedings{rao2011context,
  Title                    = {Context-aware features for singing voice detection in polyphonic music},
  Author                   = {Rao, Vishweshwara and Gupta, Chitralekha and Rao, Preeti},
  Booktitle                = {International Workshop on Adaptive Multimedia Retrieval},
  Year                     = {2011},
  Organization             = {Springer},
  Pages                    = {43--57}
}

@InProceedings{rocamora2007comparing,
  Title                    = {Comparing audio descriptors for singing voice detection in music audio files},
  Author                   = {Rocamora, Mart{\i}n and Herrera, Perfecto},
  Booktitle                = {Brazilian Symposium on Computer Music, 11th. San Pablo, Brazil},
  Year                     = {2007},
  Pages                    = {27},
  Volume                   = {26}
}

@MastersThesis{ryynanen2004probabilistic,
  Title                    = {Probabilistic modelling of note events in the transcription of monophonic melodies},
  Author                   = {Ryyn{\"a}nen, Matti},
  Year                     = {2004}
}

@Article{salamon2012melody,
  Title                    = {Melody extraction from polyphonic music signals using pitch contour characteristics},
  Author                   = {Salamon, Justin and G{\'o}mez, Emilia},
  Journal                  = {IEEE Transactions on Audio, Speech, and Language Processing},
  Year                     = {2012},
  Number                   = {6},
  Pages                    = {1759--1770},
  Volume                   = {20},

  Publisher                = {IEEE}
}

@Article{salamon2014melody,
  Title                    = {Melody Extraction from Polyphonic Music Signals: Approaches, Applications and Challenges},
  Author                   = {Justin Salamon and G{\'o}mez, Emila and Ellis, Dan and Richard, Ga{\"e}l},
  Journal                  = {IEEE Signal Processing Magazine},
  Year                     = {2014},

  Month                    = {02/2014},
  Pages                    = {118-134},
  Volume                   = {31},

  Abstract                 = {Melody extraction algorithms aim to produce a sequence of frequency values corresponding to the pitch of the dominant melody from a musical recording. Over the past decade melody extraction has emerged as an active research topic, comprising a large variety of proposed algorithms spanning a wide range of techniques. This article provides an overview of these techniques, the applications for which melody extraction is useful, and the challenges that remain. We start with a discussion of {\textquoteleft}melody{\textquoteright} from both musical and signal processing perspectives, and provide a case study which interprets the output of a melody extraction algorithm for specific excerpts. We then provide a comprehensive comparative analysis of melody extraction algorithms based on the results of an international evaluation campaign. We discuss issues of algorithm design, evaluation and applications which build upon melody extraction. Finally, we discuss some of the remaining challenges in melody extraction research in terms of algorithmic performance, development, and evaluation methodology.},
  Doi                      = {0.1109/MSP.2013.2271648}
}

@Article{Salor2007580,
  Title                    = {{Turkish} speech corpora and recognition tools developed by porting
 SONIC: Towards multilingual speech recognition },
  Author                   = {\"{O}zg\"{u}l Salor and Bryan L Pellom and Tolga \c{C}ilo\u{g}lu and M\"{u}beccel Demirekler},
  Journal                  = {Computer Speech and Language },
  Year                     = {2007},
  Number                   = {4},
  Pages                    = {580 - 593},
  Volume                   = {21},

  Doi                      = {http://dx.doi.org/10.1016/j.csl.2007.01.001},
  ISSN                     = {0885-2308},
  Keywords                 = {Phonetic aligner}
}

@InProceedings{schluter2016learning,
  Title                    = {Learning to pinpoint singing voice from weakly labeled examples},
  Author                   = {Schl{\"u}ter, Jan},
  Booktitle                = {Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR 2016)},
  Year                     = {2016},
  Pages                    = {44--50}
}

@InProceedings{sertanComposition,
  Title                    = {Composition Identification},
  Author                   = {Senturk}
}

@InProceedings{Serra_A_multicultural,
  Title                    = {{A multicultural approach in Music Information Research}},
  Author                   = {Serra, Xavier},
  Booktitle                = {{Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR 2011)}},
  Year                     = {2011},

  Address                  = {Miami, USA},
  Month                    = oct,
  Pages                    = {151--156},

  Owner                    = {joro},
  Timestamp                = {2014.04.09.18}
}

@TechReport{Serra89asystem,
  Title                    = {A System for Sound Analysis/Transformation/Synthesis Based on a Deterministic Plus Stochastic Decomposition},
  Author                   = {Xavier Serra},
  Year                     = {1989}
}

@article{serra1990spectral,
  title={Spectral modeling synthesis: A sound analysis/synthesis system based on a deterministic plus stochastic decomposition},
  author={Serra, Xavier and Smith, Julius},
  journal={Computer Music Journal},
  volume={14},
  number={4},
  pages={12--24},
  year={1990},
  publisher={JSTOR}
}

@Misc{serra2013roadmap,
  Title                    = {Roadmap for music information research},

  Author                   = {Serra, Xavier and Magas, Michela and Benetos, Emmanouil and Chudy, Magdalena and Dixon, Simon and Flexer, Arthur and G{\'o}mez, Emilia and Gouyon, Fabien and Herrera, Perfecto and Jorda, Sergi and others},
  Year                     = {2013},

  Publisher                = {MIRES Consortium}
}

@InProceedings{sordo2012musically,
  Title                    = {A Musically aware system for browsing and interacting with audio
 music collections},
  Author                   = {Sordo, Mohamed and Koduri, Gopala Krishna and Sent{\"u}rk, Sertan
 and Gulati, Sankalp and Serra, Xavier},
  Booktitle                = {Serra X, Rao P, Murthy H, Bozkurt B, editors. Proceedings of the
 2nd CompMusic Workshop; 2012 Jul 12-13; Istanbul, Turkey. Barcelona:
 Universitat Pompeu Fabra; 2012.},
  Year                     = {2012},
  Organization             = {Universitat Pompeu Fabra},

  Review                   = {distance measures are needed to navigate through the different information
 objects. 
 
 It is desired that the system is able to explore all the musical elements
 of a collection of music. An example is searching by a musical phrase,
 rhythmic pattern or an expressive articulation of vocals. The latter
 is a musical aspect that is dependent on the lyrics to the song.
 To enable a comparison between a query of a certain articulation
 of vocals and a set of target articulations, a musically-aware similarity
 metric has to be defined.
 
 
 --- 
 
 features extracted so far.}
}

@Article{sun2016personalized,
  Title                    = {Personalized, Cross-lingual TTS Using Phonetic Posteriorgrams},
  Author                   = {Sun, Lifa and Wang, Hao and Kang, Shiyin and Li, Kun and Meng, Helen},
  Year                     = {2016},
  Pages                    = {322--326},

  Booktitle                = {In Proceedings of Interspeech 2016}
}

@Article{sundberg2006kth,
  Title                    = {The {KTH} synthesis of singing},
  Author                   = {Sundberg, Johan},
  Journal                  = {Advances in {Cognitive} Psychology},
  Year                     = {2006},
  Number                   = {2-3},
  Pages                    = {131--143},
  Volume                   = {2},

  Publisher                = {Wy{\.z}sza Szko{\l}a Finans{\'o}w i Zarz{\k{a}}dzania w Warszawie}
}

@Article{sundberg1990science,
  Title                    = {The science of singing voice},
  Author                   = {Sundberg, Johan and Rossing, Thomas D},
  Journal                  = {Journal of the Acoustical Society of America},
  Year                     = {1990},
  Number                   = {1},
  Pages                    = {462--463},
  Volume                   = {87},

  Publisher                = {ASA}
}

@InProceedings{szoke2005comparison,
  Title                    = {Comparison of keyword spotting approaches for informal continuous speech.},
  Author                   = {Sz{\"o}ke, Igor and Schwarz, Petr and Matejka, Pavel and Burget, Luk{\'a}s and Karafi{\'a}t, Martin and Fapso, Michal and Cernock{\`y}, Jan},
  Booktitle                = {Interspeech},
  Year                     = {2005},
  Pages                    = {633--636}
}

@InProceedings{TurnbullEtAl_2007_ASupeApprFor,
  Title                    = {A Supervised Approach for Detecting Boundaries in Music Using Difference Features and Boosting},
  Author                   = {Turnbull, Douglas and Lanckriet, Gert and Pampalk, Elias and Goto, Masataka},
  Booktitle                = {Proceedings of the 8th International Conference on Music Information Retrieval},
  Year                     = {2007},

  Address                  = {Vienna, Austria},
  Month                    = {September 23-27},
  Pages                    = {51--54}
}

@InProceedings{uyar2014corpus_dlfm,
  Title                    = {{A corpus for computational research of Turkish makam music}},
  Author                   = {Uyar, Burak and Atl{{\i}}, Hasan Sercan and {\c{S}}ent{\"{u}}rk, Sertan and Bozkurt, Bar{{\i}}{\c{s}} and Serra, Xavier},
  Booktitle                = {1st International Digital Libraries for Musicology Workshop},
  Year                     = {2014},

  Address                  = {London, United Kingdom},
  Pages                    = {57--63},

  Abstract                 = {Each music tradition has its own characteristics in terms of melodic, rhythmic and timbral properties as well as semantic understandings. To analyse, discover and explore these culture-specific characteristics, we need music collections which are representative of the studied aspects of the music tradition. For Turkish makam music, there are various resources available such as audio recordings, music scores, lyrics and editorial metadata. However, most of these resources are not typically suited for computational analysis, are hard to access, do not have sufficient quality or do not include adequate descriptive information. In this paper we present a corpus of Turkish makam music created within the scope of the CompMusic project. The corpus is intended for computational research and the primary considerations during the creation of the corpus reflect some criteria, namely, purpose, coverage, completeness, quality and re-usability. So far, we have gathered approximately 6000 audio recordings, 2200 music scores with lyrics and 27000 instances of editorial metadata related to Turkish makam music. The metadata include information about makams, recordings, scores, compositions, artists etc. as well as the interrelations between them. In this paper, we also present several test datasets of Turkish makam music. Test datasets contain manual annotations by experts and they provide ground truth for specific computational tasks to test, calibrate and improve the research tools. We hope that this research corpus and the test datasets will facilitate academic studies in several fields such as music information retrieval and computational musicology.},
  Doi                      = {10.1145/2660168.2660174},
  ISBN                     = {9781450330022},
  Owner                    = {joro},
  Timestamp                = {2017.04.19}
}

@InProceedings{von2010perceptual,
  Title                    = {Perceptual audio features for unsupervised key-phrase detection},
  Author                   = {Von Zeddelmann, Dirk and Kurth, Frank and M{\"u}ller, M},
  Booktitle                = {Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on},
  Year                     = {2010},
  Organization             = {IEEE},
  Pages                    = {257--260},

  Review                   = {Approach to find occurences of key phrases from speech. 
Speaker independent and unsupervised. 

Uses as query spoken phrase, not textual query. 
 
features: HFCC 

search strategy : the diagonal matching [4] - line segements assuming same duration. 
it can cope with 10% change of speed}
}

@InProceedings{wang2004lyrically,
  Title                    = {LyricAlly: automatic synchronization of acoustic musical signals and textual lyrics},
  Author                   = {Wang, Ye and Kan, Min-Yen and Nwe, Tin Lay and Shenoy, Arun and Yin, Jun},
  Booktitle                = {Proceedings of the 12th annual ACM international conference on Multimedia},
  Year                     = {2004},
  Organization             = {ACM},
  Pages                    = {212--219},

  Review                   = {three modules: 

beat detector -> chorus Detector -> vocal detector 

Section 4.3. Vocal detector. 
Based on HMM of vocal only and non-vocal spectral distribution. Time resolution: inter-beat interval.

Section 5.1 Section PRocessor
Text processing is done (longest subsequence) to label chorus sections. 
phoneme durations learned from annotated sining voice dataset. duraiton is modeled by a distrubution of tis instances.

Section 5.2. bar-detector decodes real-time duration of bars and fits lyrics into these bars.}
}

@InProceedings{whiteley:06:ismir,
  Title                    = {{Bayesian modelling of temporal structure in musical audio}},
  Author                   = {Whiteley, Nick and Cemgil, Ali Taylan and Godsill, Simon},
  Booktitle                = {{Proceedings of the 7th International Society for Music Information Retrieval Conference (ISMIR 2006)}},
  Year                     = {2006},

  Address                  = {Victoria, Canada},
  Month                    = oct,
  Pages                    = {29--34},

  Owner                    = {Ajay},
  Timestamp                = {2016.01.04}
}

@Book{wichmann1991listening,
  Title                    = {Listening to theatre: the aural dimension of Beijing opera},
  Author                   = {Wichmann, Elizabeth},
  Publisher                = {University of Hawaii Press},
  Year                     = {1991}
}

@InProceedings{wiggins2009semantic,
  Title                    = {Semantic gap?? Schemantic schmap!! Methodological considerations in the scientific study of music},
  Author                   = {Wiggins, Geraint A},
  Booktitle                = {Multimedia, 2009. ISM'09. 11th IEEE International Symposium on},
  Year                     = {2009},
  Organization             = {IEEE},
  Pages                    = {477--482}
}

@Article{wong2007automatic,
  Title                    = {Automatic lyrics alignment for Cantonese popular music},
  Author                   = {Wong, Chi Hang and Szeto, Wai Man and Wong, Kin Hong},
  Journal                  = {Multimedia Systems},
  Year                     = {2007},
  Number                   = {4-5},
  Pages                    = {307--323},
  Volume                   = {12},

  Publisher                = {Springer}
}

@InProceedings{yeh2009expected,
  Title                    = {The expected amplitude of overlapping partials of harmonic sounds},
  Author                   = {Yeh, Chunghsin and R{\"o}bel, Axel},
  Booktitle                = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  Year                     = {2009},
  Organization             = {IEEE},
  Pages                    = {3169--3172}
}

@Book{young1993htk,
  Title                    = {The HTK hidden Markov model toolkit: Design and philosophy},
  Author                   = {Young, Steve J},
  Year                     = {1993}
}

@Article{yu2010hidden,
  Title                    = {Hidden semi-{Markov} models},
  Author                   = {Yu, Shun-Zheng},
  Journal                  = {Artificial Intelligence},
  Year                     = {2010},
  Number                   = {2},
  Pages                    = {215--243},
  Volume                   = {174},

  Publisher                = {Elsevier}
}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:note transcription\;0\;mcnab1995signal\;molina2015sipt
h\;;
}

