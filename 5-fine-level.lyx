#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass memoir
\begin_preamble
\definecolor{BLACK}{gray}{0}
\setsecnumdepth{subsection}
\usepackage[automake]{glossaries}
\makeglossaries

%\newacronym{cd}{CD}{compact disk}

\newglossaryentry{LAA}{type=\acronymtype, name={LAA}, description={Lyrics-to-Audio Alignment}, first={Lyrics-to-Audio Alignment (LAA)} }
\newglossaryentry{MIR}{type=\acronymtype, name={MIR}, description={Music Information Retrieval}, first={Music Information Retrieval (MIR)} }
\newglossaryentry{OTMM}{type=\acronymtype, name={OTMM}, description={Ottoman Turkish Makam Music}, first={Ottoman Turkish Makam Music (OTMM)} }
\newglossaryentry{MFCC}{type=\acronymtype, name={OTMM}, description={Mel Frequency Cepstral Coefficients}, first={Mel Frequency Cepstral Coefficients (MFCC)} }


%%%%%%%%%% Document begins %%%%%%%%%%


%\usepackage[nodoi]{apacite}
%\bibliographystyle{myapacite} 

%\definecolor{LinkColor}{rgb}{0, 0, 0.3} 
%\definecolor{ExtLinkColor}{rgb}{0, 0.3, 0}

%\usepackage[table,xcdraw]{xcolor}
%\usepackage{colortbl} % For some nice looking tables, assumes xcolor is already loaded
\end_preamble
\options noend,nofillcomment,figure, a4paper
\use_default_options false
\maintain_unincluded_children true
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "lmodern" "default"
\font_sans "helvet" "Helvetica"
\font_typewriter "tgcursor" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts true
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\output_sync_macro "\synctex=1"
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref true
\pdf_title "Knowledge-based probabilistic modeling for tracking lyrics in music audio signals"
\pdf_author "Georgi Dzhambazov"
\pdf_subject "PhD thesis"
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref page
\pdf_pdfusetitle false
\pdf_quoted_options "citecolor=blue,linkcolor=blue"
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\boxbgcolor #283d86
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 20page%
\rightmargin 20page%
\secnumdepth 3
\tocdepth 2
\paragraph_separation skip
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily\small},captionpos=b"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter

\color black
Lyrics-to-audio Alignment with Fine-level Complementary Context
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard

\color black
In this chapter, we propose how to improve the baseline lyrics-to-audio
 alignment method by considering facets of fine-level context, complementary
 to lyrics.
 We focus on one particular fine-level facet â€” the accents in the metrical
 cycle (i.e.
 metrical accents).
 In sung voice, transitions between consecutive lyrics units are aligned
 with the metrical accents to a certain degree.
 However, we found that it is not obvious how to conceptualize the direct
 relation of metrical accents to syllable transitions.
 Instead, we investigate the relation of metrical accents to the locations
 of onsets (attacks) of sung notes in the vocal melody.
 In this way, the influence of metrical events on syllable transitions is
 represented implicitly through its influence on note onsets, which are
 in turn influenced by metrical events.
 In this sense, metrical accents can be considered a facet of complementary
 context of lyrics.
\end_layout

\begin_layout Standard

\color black
With this motivation, we propose in the first part of the chapter a vocal
 onset detector that considers the simultaneously occurring accents in a
 metrical cycle.
 Vocal onset detection can be seen as a subtask of singing voice transcription.
 That is why we propose how to extend a state of the art probabilistic model
 for singing voice transcription, in which a priori probability of a note
 at a specific position in the metrical cycle interacts with the probability
 of observing a vocal note onset.
 Designing in a compact manner meter-aware transition probabilities between
 consecutive notes is the first major contribution of this chapter.
 
\end_layout

\begin_layout Standard

\color black
In the second part of the chapter, we address the relation of the transitions
 between consecutive phonemes to the simultaneously occurring vocal onsets.
 A well-known fact is that when singing voice advances from the current
 syllable to another one, simultaneously with the change of timbre a vocal
 onset is perceived 
\begin_inset CommandInset citation
LatexCommand citep
key "sundberg1990science"

\end_inset

.
 That is to say, the first voiced sound in a syllable bears the onset of
 a new note.
 
\color inherit
The second major contribution of this chapter is conceptualizing onset-aware
 phoneme transition rules
\color black
, 
\color inherit
because such relations between vocal onsets and phonemes have not been previousl
y formalized in a computational study.

\color black
 We propose further how to integrate these rules into the
\color inherit
 transition model of a phonetic recognizer.
 This contributes to alignment based on knowledge about the vocal onsets.

\color black
  To test the feasibility of the proposed model, we aligned the lyrics utilizing
 manually annotated onsets.
 
\color inherit
Further, we explore how automatically detected vocal onsets can replace
 the annotations.
 Using automatic singing transcription to detect the vocal onsets instead
 of score-informed methods reduces the need for music scores.
 
\color black
Evaluation is carried out on 
\emph on
a cappella
\emph default
 material from 
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{OTMM}
\end_layout

\end_inset


\color black
.
 
\end_layout

\begin_layout Standard

\color black
We start this chapter off by reviewing existing methods for singing voice
 transcription and existing methods for tracking metrical accents (in particular
 tracking beats) (Section 
\color inherit

\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Background-on-automatic"

\end_inset


\color black
).
 In Section 
\color inherit

\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Beat-aware-note-onset"

\end_inset

 we explore how the accuracy of vocal onset detection can be increased by
 simultaneously tracking the beats in a metrical cycle.

\color black
 Finally, in Section 
\color inherit

\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Onset-aware-lyrics-to-audio-alig"

\end_inset


\color black
 we present a study of how the detected note onsets influence the transitions
 between consecutive phonemes.
 The novel phoneme transition rules and their integration into the transitions
 of an 
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{HMM}
\end_layout

\end_inset


\color black
 are presented respectively in Sections 
\color inherit

\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Phoneme-transition-rules"

\end_inset


\color black
 and 
\color inherit

\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Transition-model"

\end_inset


\color black
.
 
\end_layout

\begin_layout Section

\color black
Background
\begin_inset CommandInset label
LatexCommand label
name "sec:Background-on-automatic"

\end_inset


\end_layout

\begin_layout Subsection

\color black
Singing voice transcription
\end_layout

\begin_layout Standard
The process of converting an audio recording into some form of music notation
 is commonly known as automatic music transcription.
 Current transcription methods use general purpose models, which are unable
 to capture the rich diversity found in music signals from different instruments
 
\begin_inset CommandInset citation
LatexCommand cite
key "benetos2013automatic"

\end_inset

.
 In particular, singing voice poses a challenge to transcription algorithms
 because of its soft onsets and expressive elements such as portamento and
 vibrato.
 
\color black
In recent years there has been a substantial amount of work on the extraction
 of pitch from both 
\emph on
a cappella
\emph default
 singing 
\begin_inset CommandInset citation
LatexCommand citep
key "babacan2013comparative,molina2014importance"

\end_inset

 and predominant singing voice from polyphonic music 
\begin_inset CommandInset citation
LatexCommand citep
key "salamon2014melody"

\end_inset

.
 This has paved the way to an increased accuracy of singing voice transcription
 algorithms.
 One of the reasons for this is that a correctly detected melody contour
 is a fundamental precondition for 
\color inherit
singing voice transcription
\color black
 (SVT).
\end_layout

\begin_layout Standard
The core subtasks of SVT are detecting note events with a discrete pitch
 value, an onset time and an offset time from the estimated time-pitch represent
ation.
 A 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{HMM}
\end_layout

\end_inset

 that describes notes is presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "ryynanen2004probabilistic"

\end_inset

, wherein a note has 3 states: attack (onset), stable pitch state and silent
 state.
 The transition probabilities are learned from data.
 Recently 
\begin_inset CommandInset citation
LatexCommand cite
key "mauch2015computer"

\end_inset

 suggested to represent the observation and transition likelihoods by rules
 compacted from music knowledge, instead of learning them from data.
 The model covers a range with distinct pitches from a lowest MIDI tone
 C2 up to B7.
 
\color black
Each MIDI pitch is further divided into 3 sub-pitches, resulting in 
\begin_inset Formula $n=207$
\end_inset

 notes with different pitch, each having the 3 note states.
 
\color inherit
Although being conceptually capable of tracking onsets in singing voice
 audio with accompaniments, these approaches were tested only on 
\emph on
a cappella
\emph default
 singing.
 In multi-instrumental recordings, an essential first step is to extract
 reliably the predominant vocal melody.
 One of the few works dealing with SVT for polyphonic recordings â€” 
\begin_inset CommandInset citation
LatexCommand citet
key "kroher2015automatic,NishikimiNIY16"

\end_inset

 â€” are based on the algorithm for predominant melody extraction of 
\begin_inset CommandInset citation
LatexCommand cite
key "salamon2012melody"

\end_inset

.
 Temporal deviations of the sung onsets from their positions indicated in
 music score are modeled in a probabilistic way in 
\begin_inset CommandInset citation
LatexCommand cite
key "NishikimiNIY16"

\end_inset

.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "kroher2015automatic"

\end_inset

 as a primary step of the note transcription stage, notes are segmented
 by a set of flamenco-specific onset detection rules, based on pitch contour
 and volume characteristics.
 
\end_layout

\begin_layout Standard
Vocal onsets are usually soft (a slower attack phrase), in contrast to some
 instruments with percussive onsets 
\begin_inset CommandInset citation
LatexCommand citep
key "sundberg1990science"

\end_inset

.
 This makes the precise location of a vocal onset ill-defined.
 
\color black
The note onset corresponds to the initial segment of the three temporal
 segments of a vocal note: attack, sustain and release
\begin_inset Foot
status open

\begin_layout Plain Layout

\color black
We stick to the definition of
\emph on
 temporal segments
\emph default
, adopted from the chapter 
\emph on
Singing Transcription
\emph default
 of 
\begin_inset CommandInset citation
LatexCommand citet
key "klapuri:06:transcription"

\end_inset


\end_layout

\end_inset

.
 As the location of a note onset we will refer to the time instant, in which
 a pitched segment starts.
 They could follow (but exclude) a region with higher energy in case a syllable
 starts with a non-voiced consonant.
 The other temporal segments â€” sustain and release (offset) have undoubtedly
 also impact on the transition of phonemes.
 
\color inherit
However, in this thesis, we consider the impact of vocal note onsets only,
 for they have arguably the most evident influence on syllable transitions.
 
\end_layout

\begin_layout Standard
The detection of instrumental onsets in polyphonic recordings is a challenging
 problem itself, which has attracted the attention of researchers for many
 years
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "www.music-ir.org/mirex/wiki/2015:Audio_Onset_Detection"

\end_inset


\end_layout

\end_inset

.
 Most algorithms are based on the observation that an onset entails a change
 of the energy of the signal or of its harmonic content.
 One successful approach is to distinguish the spectral peaks, which are
 due to candidate note transient time segments 
\begin_inset CommandInset citation
LatexCommand citep
key "roebel2009onset"

\end_inset

.
 Soft onsets are treated as a special case: the sensitivity of the generic
 transient detector is modified whenever the transients appear in a harmonic
 structure, which is usually a condition for soft onsets.
 A thorough review of onset detection methods can be found in 
\begin_inset CommandInset citation
LatexCommand citet
after "Section 2.2"
key "benetos2013automatic"

\end_inset

.
 Vocal onset detection in multi-instrumental music is, in fact, one of the
 hardest 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{MIR}
\end_layout

\end_inset

 problems.
 Determining their exact onset timestamp is even harder in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{OTMM}
\end_layout

\end_inset

 because of expressive singing phenomena: vocal onsets are often approached
 by portamentos.
 Therefore any complementary information can be an important 'stepping stone'
 for increased detection accuracy.
\end_layout

\begin_layout Subsection
Beat Detection
\end_layout

\begin_layout Standard
Recently a Bayesian approach, referred to as the 
\emph on
bar-pointer
\emph default
 model, has been presented 
\begin_inset CommandInset citation
LatexCommand citep
key "whiteley:06:ismir"

\end_inset

.
 It describes events in music as being driven by their current position
 in a metrical cycle (i.e.
 musical bar).
 
\color black
The model represents as hidden variables in a 
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{DBN}
\end_layout

\end_inset


\color black
 the current position in a bar, the tempo, and the type of musical meter,
 which can be referred to as bar-tempo state space.
\end_layout

\begin_layout Standard
The work of 
\begin_inset CommandInset citation
LatexCommand citet
key "holzapfel2014tracking"

\end_inset

 applied this model to recordings from non-Western music, in order to handle
 jointly beat and downbeat tracking.
 The authors showed that the original model can be adapted to different
 rhythmic styles and time signatures, and an evaluation is presented on
 Indian, Cretan and Turkish music datasets.
 
\end_layout

\begin_layout Standard

\color black
Later 
\begin_inset CommandInset citation
LatexCommand citet
key "krebs:15:ismir"

\end_inset

 suggested a modification of the bar-tempo state space, in order to reduce
 the computational burden from its huge size.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section

\color black
Metrical-accent-aware vocal onset detection
\begin_inset CommandInset label
LatexCommand label
name "sec:Beat-aware-note-onset"

\end_inset


\end_layout

\begin_layout Standard

\color black
Metrical accents are a facet of complementary context that defines the rhythmic
 backbone of vocal melodies.
 Therefore it is worth studying how the transitions between lyrics of units
 (in particular syllables) interact with these accents.
 
\color inherit
By 
\emph on
metrical accents
\emph default
 we will refer to notes that are emphasized as a result of the context of
 the musical meter.
 Naturally, accents occur on the beats, whereby downbeats (the first beat
 in a meter) will be perceived as being stronger accentuated.
 Detecting the times of vocal note onsets can benefit from automatically
 detected metrical events, such as beats.
 
\color black
In fact, the accents in a metrical cycle determine to a large extent the
 temporal backbone of the singing melody.
 
\color inherit
Studies on symbolic music data showed that the timestamps of vocal note
 onsets are influenced by the their position in a metrical cycle 
\begin_inset CommandInset citation
LatexCommand citep
key "huron2006sweet,holzapfel2015relation"

\end_inset

.
 Despite that, there have been very few studies on meter-aware analysis
 of onsets in music audio 
\begin_inset CommandInset citation
LatexCommand citep
key "degara2010note"

\end_inset

.
 
\end_layout

\begin_layout Standard
In this section we make a hypothesis that the knowledge of the current position
 in a metrical cycle (i.e.
 metrical accent) can improve the accuracy of vocal note onset detection.
 To this end we propose a novel probabilistic model to jointly track beats
 and vocal note onsets.
 
\end_layout

\begin_layout Subsection
Model Architecture
\end_layout

\begin_layout Standard
The proposed approach extends the beat and meter tracking model, presented
 in 
\begin_inset CommandInset citation
LatexCommand citet
key "krebs:15:ismir"

\end_inset

.
 
\color black
We adopt from it the variables for the position in the metircal cycle (bar
 position) 
\begin_inset Formula $\phi$
\end_inset

 and the instantaneous tempo 
\color inherit

\begin_inset Formula $\dot{\phi}$
\end_inset

.
 We also adopt the observation model, which describes how
\color black
 the metrical accents (beats) are related to an observed onset feature vector
 
\begin_inset Formula $y_{f}$
\end_inset

.
 
\color inherit
All variables and their conditional dependencies are represented as the
 hidden variables in a 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{DBN}
\end_layout

\end_inset

 (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "dbn"

\end_inset

).
 We consider that the 
\emph on
a priori
\emph default
 probability of a note at a specific metrical accent interacts with the
 probability of observing a vocal note onset.
 To represent that interaction we add a hidden state for the temporal segment
 of a vocal note 
\emph on
n
\emph default
, which depends on the current position in the metrical cycle.
 The probability of observing a vocal onset
\color black
 is derived from the emitted pitch 
\begin_inset Formula $y_{p}$
\end_inset

 of the vocal melody.

\color inherit
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../pypYIN/report/ISMIR_2017/note_bar_DBN.eps
	lyxscale 10
	width 60page%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A dynamic Bayesian network for the proposed beat and vocal onset detection
 model.
 Circles and squares denote continuous and discrete variables, respectively.
 Gray nodes and white nodes represent observed and hidden variables, respectivel
y.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "dbn"

\end_inset


\end_layout

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
the figure can be found in the repo https://github.com/georgid/pypYIN/tree/master
/report/ISMIR_2017
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the proposed 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{DBN}
\end_layout

\end_inset

, an observed sequence of features derived from an audio signal 
\begin_inset Formula $y_{1:K}=\{y,..,y_{K}\}$
\end_inset

 is generated by a sequence of hidden (unknown) variables 
\begin_inset Formula $x_{1:K}=\{x_{1},...,x_{K}\}$
\end_inset

, where K is the length of the sequence (number of audio frames in an audio
 excerpt).
 The joint probability distribution of hidden and observed variables factorizes
 as: 
\begin_inset Formula 
\begin{equation}
P(x_{1:K},y_{1:K})=P(x_{0})\Pi_{k=1}^{K}P(x_{k}|x_{k-1})P(y_{k}|x_{k})\label{eq:inference-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $P(x_{0})$
\end_inset

 is the initial state distribution; 
\begin_inset Formula $P(x_{k}|x_{k-1})$
\end_inset

 is the transition model and 
\begin_inset Formula $P(y_{k}|x_{k})$
\end_inset

 is the observation model.
\end_layout

\begin_layout Subsection
Hidden variables
\end_layout

\begin_layout Standard
At each audio frame 
\begin_inset Formula $k$
\end_inset

, the hidden variables describe the state of a hypothetical bar pointer
 
\begin_inset Formula $x_{k}=[\dot{\phi_{k}},\phi_{k},n_{k}]$
\end_inset

, representing the instantaneous tempo, the bar position and the vocal note
 respectively.
 
\end_layout

\begin_layout Subsubsection
Tempo state 
\begin_inset Formula $\dot{\phi}$
\end_inset

 and bar position state 
\begin_inset Formula $\phi$
\end_inset

 
\end_layout

\begin_layout Standard

\color black
The bar position 
\begin_inset Formula $\phi$
\end_inset

 points to the current position in the metrical cycle (bar).
 The instantaneous tempo 
\begin_inset Formula $\dot{\phi}$
\end_inset

 encodes how many bar positions the pointer advances from the current to
 the next time instant.
 To assure feasible computational time we relied on the combined bar-tempo
 efficient state space, presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "krebs:15:ismir"

\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout

\color black
The total number of bar positions 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $|\phi|$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color black
 is derived by multiplying b by the number of beats in a metrical cycle.
 
\end_layout

\end_inset

To keep the size of the bar-tempo state space small, we input the ground
 truth tempo for each recording, allowing a range for 
\color inherit

\begin_inset Formula $\dot{\phi}$
\end_inset


\color black
 within 
\begin_inset Formula $\pm10$
\end_inset

 bpm from it, in order to accommodate gradual tempo changes.
 This was the minimal margin at which beat tracking accuracy did not degrade
 substantially.
 For a study with data with higher stylistic diversity, it would make sense
 to increase it to at least 20% as it is done in 
\begin_inset CommandInset citation
LatexCommand cite
after "Section 5.2"
key "holzapfel2016bayesian"

\end_inset

.
 This yields around 100-1000 states for the bar positions within a single
 beat (in the order of 
\begin_inset Formula $10000$
\end_inset

 for the 8-9 beats of the usuls ).
\end_layout

\begin_layout Subsubsection
Vocal note state 
\begin_inset Formula $n$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "subsec:Note-state"

\end_inset


\end_layout

\begin_layout Standard
The vocal note states represent the temporal segments of a sung note.
 They are a modified version of these suggested in the note transcription
 model of 
\begin_inset CommandInset citation
LatexCommand cite
key "mauch2015computer"

\end_inset

.
 We adopted the first two segments: attack region (A), stable pitch region
 (S).
 We replaced the silent segment with non-vocal state (N).
 Because full-fledged note transcription is outside the scope of this work,
 instead of 3 steps per semitone, we used for simplicity only a single one,
 which deteriorated just slightly the note onset detection accuracy.
 Also, to reflect the pitch range in the datasets, on which we evaluate,
 we set as minimal MIDI note E3 covering almost 3 octaves up to B5 (35 semitones
).
 This totals to 105 note states.
 
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
To be able to represent the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{DBN}
\end_layout

\end_inset

 as an 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{HMM}
\end_layout

\end_inset

, the bar-tempo efficient state space is combined with the note state space
 into a joint state space 
\emph on
x
\emph default
.
 The joint state space is a cartesian product of the two state spaces, resulting
 in up to 
\begin_inset Formula $10000$
\end_inset


\begin_inset Formula $\times105\approx1\thinspace$
\end_inset

M states.
\end_layout

\begin_layout Subsection
Transition model
\end_layout

\begin_layout Standard
Due to the conditional dependence relations in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "dbn"

\end_inset

 the transitional model factorizes as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{array}{cc}
P(x_{k}|x_{k-1})= & P(\dot{\phi}_{k}|\dot{\phi}_{k-1})\thinspace\times\\
P(\phi_{k}|\phi{}_{k-1},\dot{\phi}_{k-1})\thinspace\times & P(n_{k}|n_{k-1},\phi_{k})
\end{array}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The tempo transition probability 
\begin_inset Formula $p(\dot{\phi_{k}}|\dot{\phi}_{k-1})$
\end_inset

 and bar position probability 
\begin_inset Formula $p(\phi_{k}|\phi_{k-1},\dot{\phi}_{k-1})$
\end_inset

 are the same as in
\color black
 
\begin_inset CommandInset citation
LatexCommand cite
key "krebs:15:ismir"

\end_inset


\color blue
.

\color black
 Transition from one tempo to another is allowed only at bar positions,
 at which the beat changes.
 This is a reasonable assumption for the local tempo deviations in the analyzed
 datasets, which can be considered to occur relatively beat-wise.
 
\end_layout

\begin_layout Subsubsection
Note transition probability 
\begin_inset CommandInset label
LatexCommand label
name "par:Note-transition-probability"

\end_inset


\end_layout

\begin_layout Standard
The probability of advancing to a next note state is based on the transitions
 of the note-HMM, introduced in 
\begin_inset CommandInset citation
LatexCommand cite
key "mauch2015computer"

\end_inset

.
 Let us briefly review it: From a given note segment the only possibility
 is to progress to its following note segment.
 To ensure continuity each of the self-transition probabilities is rather
 high, given by constants 
\emph on

\begin_inset Formula $c_{A}$
\end_inset

, 
\begin_inset Formula $c_{S}$
\end_inset

 
\emph default
and 
\emph on

\begin_inset Formula $c_{N}$
\end_inset

 
\emph default
for A, S and N segments respectively (
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $c_{A}$
\end_inset

=0.9
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
; 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $c_{S}$
\end_inset

=0.99
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
; 
\begin_inset Formula $c_{N}=0.9999$
\end_inset

).
 Let 
\begin_inset Formula $P_{N_{i}A_{j}}$
\end_inset

 be the probability of transition from non-vocal state 
\begin_inset Formula $N_{i}$
\end_inset

 after note 
\begin_inset Formula $i$
\end_inset

 to attack state 
\begin_inset Formula $A_{j}$
\end_inset

 of its following note 
\begin_inset Formula $j$
\end_inset

.

\color red
 
\color inherit
The authors assume that it depends on the difference between the pitch values
 of notes 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

 and it can be approximated by a normal distribution centered at change
 of zero (
\begin_inset CommandInset citation
LatexCommand cite
key "mauch2015computer"

\end_inset

, Figure 1.b).
 This implies that small pitch changes are more likely than larger ones.
 Now we can formalize their note transition as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(n_{k}|n_{k-1})=\begin{cases}
P_{N_{i}A_{j}}, & n_{k-1}=N_{i}\quad n_{k}=A_{j}\\
c_{N}, & n_{k-1}=n_{k}=N_{i}\\
1-c_{A}, & n_{k-1=}A_{i}\quad n_{k}=S_{j}\\
c_{A}, & n_{k-1}=n_{k}=A_{i}\\
1-c_{S} & n_{k=1}=S_{i\quad}n_{k}=N_{j}\\
c_{S}, & n_{k-1}=n_{k}=S_{i}\\
0 & otherwise
\end{cases}\label{eq:note_trans_probs}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Note that the outbound transitions from all non-vocal states 
\begin_inset Formula $N_{i}$
\end_inset

 should sum to 1, meaning that
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula 
\begin{equation}
c_{N}=1-\sum_{i}P_{N_{i}A_{j}}\label{eq:normalization_trans_prob}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\color black
In this study, we modify 
\begin_inset Formula $P_{N_{i}A_{j}}$
\end_inset

 to allow variation
\color inherit
 in time, depending on the current bar position 
\begin_inset Formula $\phi_{k}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(n_{k}|n_{k-1,}\phi_{k})=\begin{cases}
P_{N_{i}A_{j}}\Theta(\phi_{k}), & n_{k-1}=N_{i},n_{k}=A_{j}\\
c_{N}, & n_{k-1}=n_{k}=N_{i}\\
...
\end{cases}\label{eq:note_onset_trans_prob}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where
\end_layout

\begin_layout Description
\begin_inset Formula $\Theta(\phi_{k}):$
\end_inset

 function weighting the contribution of a beat adjacent to current bar position
 
\begin_inset Formula $\phi_{k}$
\end_inset


\end_layout

\begin_layout Standard
and
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\begin{equation}
c_{N}=1-\Theta(\phi_{k})\sum_{i}P_{N_{i}A_{j}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\color black
The transition probabilities in all the rest of the 
\color inherit
cases remain the same.
 We explore two variants of the weighting function 
\begin_inset Formula $\Theta(\phi_{k}):$
\end_inset

 
\end_layout

\begin_layout Standard

\series bold
1.
 Time-window redistribution weighting:
\series default
 Singers often advance or delay slightly note onsets off the location of
 a beat.
 The work 
\begin_inset CommandInset citation
LatexCommand cite
key "NishikimiNIY16"

\end_inset

 presented an idea on how to model vocal onsets, time-shifted from a beat,
 by stochastic distribution.
 Similarly, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
we introduce 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
a normal distribution 
\begin_inset Formula $\mathcal{N}{}_{0,\sigma}$
\end_inset

, centered around 0 to re-distribute the importance of a metrical accent
 (beat) over a time window around it.
 Let 
\begin_inset Formula $b_{k}$
\end_inset

 be 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
the 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
beat, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
closest
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 in time to a current bar position 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\phi_{k}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
.
 Now:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\begin{equation}
\Theta(\phi_{k})=[\mathcal{N}_{0,\sigma}(d(\phi_{k},b_{k}))]^{w}e(b_{k})\label{eq:weight_anno}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
where
\end_layout

\begin_layout Description
\begin_inset Formula $e(b):$
\end_inset

 probability of a note onset co-occurring with the 
\begin_inset Formula $b^{th}$
\end_inset

 beat (b 
\begin_inset Formula $\in$
\end_inset


\begin_inset Formula $B$
\end_inset

); 
\begin_inset Formula $B$
\end_inset

 is the number of beats in a metrical cycle
\end_layout

\begin_layout Description
\begin_inset Formula $w:$
\end_inset

 sensitivity of vocal onset probability to beats 
\end_layout

\begin_layout Description
\begin_inset Formula $d(\phi_{k},b_{k}):$
\end_inset

 the distance from current bar position 
\begin_inset Formula $\phi_{k}$
\end_inset

 to the position of the closest beat 
\begin_inset Formula $b_{k}$
\end_inset

 
\end_layout

\begin_layout Standard
Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:note_onset_trans_prob"

\end_inset

 means essentially that the original 
\begin_inset Formula $P_{N_{i}A_{j}}$
\end_inset

 is scaled according to how close in time to a beat it is.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
impl (`code.MonoNoteParameters.barPositionDist_Probs`)
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
2.
 Simple weighting:
\series default
 We also aim at testing a more conservative hypothesis that it is sufficient
 to approximate the influence of metrical accents only at the locations
 of beats.
 To reflect that, we 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
modify the 
\begin_inset Formula $P_{N_{i}A_{j}}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 only at bar positions corresponding to beat positions, for which the weighting
 function is set to the peak of 
\begin_inset Formula $N_{0,\sigma}$
\end_inset

, and to 1 elsewhere.
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\begin{equation}
\Theta(\phi_{k})=\begin{cases}
[N_{0,\sigma}(0)]^{w}e(b_{k}), & d(\phi_{k},b_{k})=0\\
1 & else
\end{cases}\label{eq:weight_detected}
\end{equation}

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
madmom.features.bar_notes_hmm.NoteTransitionModel._construct_trans_probs
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Observation models
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
The observation probability 
\begin_inset Formula $P(y_{k}|x_{k})$
\end_inset

 describes the relation between the hidden states and the (observed) audio
 signal.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 In this work we make the assumption that the observed vocal pitch and the
 observed metrical accent are conditionally independent from each other.
 This assumption may not hold in cases when energy accents of singing voice,
 which contribute to the total energy of the signal, are correlated to changes
 in pitch.
 However, for music with percussive instruments the importance of singing
 voice accents is diminished to a significant extent by percussive accents.
 Now we can rewrite Eq.
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:inference"

\end_inset

 as 
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Formula 
\begin{equation}
\begin{array}{cc}
P(x_{1:K},y_{1:K}^{f},y_{1:K}^{p})=\\
P(x_{0})\Pi_{k=1}^{K}P(x_{k}|x_{k-1})P(y_{k}^{f}|x_{k})P(y_{k}^{p}|x_{k})
\end{array}
\end{equation}

\end_inset


\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{equation}
\end_layout

\begin_layout Plain Layout


\backslash
begin{array}{cc}
\end_layout

\begin_layout Plain Layout

P(x_{1:K},y_{1:K}^{f},y_{1:K}^{p})=
\backslash

\backslash
[3pt]
\end_layout

\begin_layout Plain Layout

P(x_{0})
\backslash
Pi_{k=1}^{K}P(x_{k}|x_{k-1})P(y_{k}^{f}|x_{k})P(y_{k}^{p}|x_{k})
\end_layout

\begin_layout Plain Layout


\backslash
end{array}
\end_layout

\begin_layout Plain Layout


\backslash
end{equation}
\end_layout

\end_inset

This means essentially that the observation probability can be represented
 as the product of the observation probability of a metrical accent 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $P(y_{k}^{f}|x_{k})$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 and the observation probability of vocal pitch 
\color black

\begin_inset Formula $P(y_{k}^{p}|x_{k})$
\end_inset


\color inherit
.
\end_layout

\begin_layout Subsubsection
Accent observation model
\end_layout

\begin_layout Standard
For 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $P(y_{k}^{f}|x_{k})$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 we train 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{GMM}
\end_layout

\end_inset

-s on the spectral flux-like feature 
\begin_inset Formula $y^{f}$
\end_inset

, extracted from the audio signal using the same parameters as in 
\begin_inset CommandInset citation
LatexCommand cite
key "krebs2013rhythmic"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "holzapfel2014tracking"

\end_inset

.
 The feature 
\begin_inset Formula $y^{f}$
\end_inset

 summarizes the energy changes (accents) that are likely to be related to
 the onsets of all instruments together.
 The probability of observing an energy change depends on the position in
 the bar and the rhythmic pattern, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $P(y_{k}^{f}|x_{k})=P(y_{k}^{f}|\phi_{k},r_{k})$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Pitch observation model
\end_layout

\begin_layout Standard

\color black
The pitch probability 
\begin_inset Formula $P(y_{k}^{p}|x_{k})$
\end_inset

 
\color inherit
reduces to 
\color black

\begin_inset Formula $P(y_{k}^{p}|n_{k})$
\end_inset


\color inherit
, because it depends only the current note state.
 We adopt the idea proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "mauch2015computer"

\end_inset

 that a vocal note state emits pitch 
\begin_inset Formula $y^{p}$
\end_inset

 according to a normal distribution, centered around its average pitch.
 The standard deviation of stable states and the one of the onset states
 are kept the same as in the original model, respectively 0.9 and 5 semitones.
 The melody contour of singing is extracted in a preprocessing step.
 We utilized an algorithm, extended from 
\begin_inset CommandInset citation
LatexCommand cite
key "salamon2012melody"

\end_inset

 and tailored to Turkish makam.
 Each audio frame 
\begin_inset Formula $k$
\end_inset

 gets assigned a pitch value and probability of being voiced 
\begin_inset Formula $v_{k}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "atli2014audio"

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
IMPL: for now it is constant PROB_PITCHED=0.9 in 
\begin_inset CommandInset href
LatexCommand href
name "PITCH_PROB"
target "https://github.com/georgid/pypYIN/blob/master/pypYIN/MonoNoteParameters.py#L47"

\end_inset


\end_layout

\end_inset

.
 Based on frames with zero probabilities, one can infer which segments are
 vocal and which not.
 Since correct vocal segments is crucial for the sake of this study and
 the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{VAD}
\end_layout

\end_inset

 of these melody extraction algorithms are not state of the art, we manually
 annotated segments with singing voice, and thus assigned 
\begin_inset Formula $v_{k}=0$
\end_inset

 for all frames, annotated as non-vocal.
\end_layout

\begin_layout Standard

\color black
For each state the observation probability 
\begin_inset Formula $P(y_{k}^{p}|n_{k})$
\end_inset

 of vocal states is normalized to sum to 
\begin_inset Formula $v_{k}$
\end_inset

 (unlike the original model which sums to a global constant v).

\color inherit
 This leaves the probability for each non-vocal state be 
\begin_inset Formula $\nicefrac{1-v_{k}}{n}$
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
IMPL: posteriorPichedProb in code.MonoNoteHMM.MonoNoteHMM.normalize_obs_probs
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Learning model parameters
\end_layout

\begin_layout Subsubsection
Accent observation model
\begin_inset CommandInset label
LatexCommand label
name "subsec:Bar-Observation-model"

\end_inset


\end_layout

\begin_layout Standard
For this study we divided the
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 the 
\family default
\series default
\shape default
\size default
\emph on
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
multi-instrumental vocal onsets OTMM dataset
\emph default
 (see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Multi-instrumental-vocal-onsets"

\end_inset

) into training and test subsets.
 The training dataset spans around 7 minutes of audio from each of the two
 usuls.
 Due to the scarcity of material with solo singing voice, several excerpts
 with choir sections were included in the training data.
 We trained the accent probability patterns 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $P(y_{k}^{f}|\phi_{k},r_{k})$
\end_inset

 on the training dataset.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 For each usul we trained one rhythmic pattern by fitting a 2-mixture 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{GMM}
\end_layout

\end_inset

 on the spectral-flux-like feature vector 
\begin_inset Formula $y^{f}$
\end_inset

.
 Analogously to 
\begin_inset CommandInset citation
LatexCommand cite
key "holzapfel2014tracking"

\end_inset

 we pooled the bar positions down to 16 patterns per beat.
 The feature vector is normalized to zero mean, unit variance and taking
 moving average.
 Normalization is done per song.
 
\end_layout

\begin_layout Subsubsection
Probability of note onset
\end_layout

\begin_layout Standard
The probability of a vocal note onset co-occurring at a given bar position
 
\begin_inset Formula $e(b)$
\end_inset

 is obtained from studies on sheet music.

\series bold
 
\series default
Many notes are aligned with a beat in the music score, meaning a higher
 probability of a note at beats compared to inter-beat bar positions.
 A separate distribution 
\begin_inset Formula $e(b)$
\end_inset

 is applied for each different metrical cycle.
 For the dÃ¼yek and aksak usuls 
\begin_inset Formula $e(b)$
\end_inset

 has been taken from a recent study 
\begin_inset CommandInset citation
LatexCommand cite
after "Figure 5. a-c"
key "holzapfel2015relation"

\end_inset

.
 The authors used a corpus of music scores, on data from the same corpus,
 from which we derived the dataset.
 The patterns reveal that notes are expected to be located with much higher
 likelihoods on those beats with percussive strokes than on the rest.
 
\end_layout

\begin_layout Subsection
Inference
\end_layout

\begin_layout Standard
We obtain the best state sequence 
\begin_inset Formula $x_{1:K}$
\end_inset

 by decoding with the Viterbi algorithm.
 A note onset is detected when the state path enters an attack note state
 after being in non-vocal state.
\end_layout

\begin_layout Subsubsection
With manually annotated beats
\begin_inset CommandInset label
LatexCommand label
name "subsec:With-manually-annotated"

\end_inset


\end_layout

\begin_layout Standard
We explored the option that beats are given as input from a preprocessing
 step (i.e.
 when they are manually annotated).
 In this case, the detection of vocal onsets can be carried out by a reduced
 model with a single hidden variable: the note state.
 The observation model is then reduced to the pitch observation probability.
 The transition model is reduced to bar-position-aware transition probability
 
\begin_inset Formula $a_{ij}(k)=p(n_{k}=j|n_{k-1}=i,\phi_{k})$
\end_inset

(see Eq.
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:note_onset_trans_prob"

\end_inset

).

\color black
 
\color inherit
To represent this
\color black
 time-dependent self-transition probabilities we we utilize time-varying
 transition matrix.
 It falls in the general category of variable-time 
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{HMM}
\end_layout

\end_inset


\color black
-s (VTHMMs) 
\begin_inset CommandInset citation
LatexCommand cite
key "johnson2005capacity"

\end_inset

.
 The standard transition probabilities in the Viterbi maximization step
 in equation 
\color inherit

\begin_inset CommandInset ref
LatexCommand ref
reference "eq:time-variable-viterbi-1"

\end_inset


\color black
 are substituted for the bar-position-aware transitions 
\begin_inset Formula $a_{ij}(k)$
\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
 To be able to control the influence of the onsets, we have introduced a
 weighting factor 
\begin_inset Formula $\gamma$
\end_inset

:
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\color black
\begin_inset Formula 
\begin{equation}
\delta_{k}(j)=\max_{i\in(j,\thinspace j-1)}\delta_{k-1}(i)\thinspace a_{ij}(k)\thinspace b_{j}(O_{k})\label{eq:time-variable-viterbi}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
Full model
\end_layout

\begin_layout Standard
In addition to onsets, a beat is detected when the bar position variable
 hits one of 
\begin_inset Formula $B$
\end_inset

 positions of beats within the metrical cycle.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
comment independence
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that the size of the state space 
\begin_inset Formula $x$
\end_inset

 poses a memory requirement.
 A recording of 1 minute has around 
\begin_inset Formula $10000$
\end_inset

 frames at a hopsize of 
\begin_inset Formula $5.8\thinspace$
\end_inset

ms.
 To use Viterbi thus requires to store in memory pointers to up to 
\begin_inset Formula $4\thinspace$
\end_inset

G states, which amounts to 
\begin_inset Formula $40\thinspace$
\end_inset

G RAM (with uint32 python data type).
\end_layout

\begin_layout Subsection
Experiments
\begin_inset CommandInset label
LatexCommand label
name "subsec:Experiments"

\end_inset


\end_layout

\begin_layout Standard
Vocal detection is evaluated on 5 1-minute excerpts from each of the two
 usuls from the 
\emph on
multi-instrumental vocal onsets OTMM dataset
\emph default
 (see Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Multi-instrumental-vocal-onsets"

\end_inset

), totaling in 10 minutes of audio (on total 780 onsets).
 The hopsize of computing the spectral flux feature, which resulted in best
 beat detection accuracy in 
\begin_inset CommandInset citation
LatexCommand cite
key "holzapfel2014tracking"

\end_inset

 is 
\begin_inset Formula $h_{f}=20\thinspace ms$
\end_inset

.
 In comparison,
\series bold
 
\series default
the hopsize of predominant vocal melody detection is usually of smaller
 order i.e.
 
\begin_inset Formula $h_{p}=5.8\thinspace ms$
\end_inset

 (corresponding to 256 frames at sampling rate of 44100).
 Preliminary experiments showed that extracting pitch with values of 
\begin_inset Formula $h_{p}$
\end_inset

 bigger than this values reasonably deteriorated the vocal onset accuracy.
 Therefore in this work we used hopsize of 5.8 ms for the extraction of both
 features.
 The time difference parameter for the spectral flux computation remains
 unaffected by this change in hopsize, because it can be set separately.
 
\end_layout

\begin_layout Standard
As a baseline we run the algorithm of 
\begin_inset CommandInset citation
LatexCommand cite
key "mauch2015computer"

\end_inset

 with the 105 note states, we introduced in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Note-state"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
We extended the port of the original VAMP plugin implementation to Python
 
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/ronggong/pypYIN"

\end_inset

, which we make available at 
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/georgid/pypYIN"

\end_inset


\end_layout

\end_inset

.
 The note transition probability is the original as presented in Eq.
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:note_trans_probs"

\end_inset

, i.e.
 not aware of beats.
 Note that in 
\begin_inset CommandInset citation
LatexCommand cite
key "mauch2015computer"

\end_inset

 the authors introduce a post-processing step, in which onsets of consecutive
 sung notes with same pitch are detected considering their intensity difference.
 We excluded this step in all system variants presented, because it could
 not be integrated in the proposed observation model in a trivial way.
 This means that, essentially, in this experiments cases of consecutive
 same-pitch notes are missed, which decreases somewhat the recall compared
 to the original algorithm.
 
\end_layout

\begin_layout Subsubsection
Evaluation metrics
\end_layout

\begin_layout Paragraph
Beat detection
\end_layout

\begin_layout Standard
Since improvement of the beat detector is outside the scope of this dissertation
, we report accuracy of detected beats only in terms of their F-measure
\begin_inset Foot
status open

\begin_layout Plain Layout
The evaluation script used is at 
\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/CPJKU/madmom/blob/master/madmom/evaluation/beats.py"

\end_inset


\end_layout

\end_inset

.
 This serves solely the sake of comparison to existing work
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Note that F-measure is agnostic to the phase of the detected beats, which
 is clearly not optimal
\end_layout

\end_inset

.
 The F-measure can take a maximum value of 1, while beats tapped on the
 off-beat relative to annotations will be assigned an F-measure of 0.
 We used the default tolerance window of 
\begin_inset Formula $70\thinspace ms$
\end_inset

, also applied in 
\begin_inset CommandInset citation
LatexCommand cite
key "holzapfel2014tracking"

\end_inset

.
 
\begin_inset Note Comment
status collapsed

\begin_layout Plain Layout
unlike vocal note onsets, beats are evaluated non only in vocal parts, but
 everything until endTime in excerpt.txt, this is not absolutely correct,
 as there might be different beat accuracy in vocal and non-vocal regions
 
\end_layout

\end_inset


\begin_inset Float table
placement t
wide true
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="6">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
meter
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
beat Fmeas
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
P
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
R
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fmeas
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
aksak
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mauch
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
33.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
31.6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
31.6
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Ex-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
37.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
38.4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
37.2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Ex-2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
86.4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
37.8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
36.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
36.1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
dÃ¼yek
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mauch
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
42.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
36.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
37.9
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Ex-1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
44.3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
41.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
41.4
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Ex-2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
72.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
45.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
39.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
40.3
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Evaluation results for Experiment 1 (shown as Ex-1) and Experiment 2 (shown
 as Ex-2).
 Mauch stands for the baseline, following the approach of 
\begin_inset CommandInset citation
LatexCommand cite
key "mauch2015computer"

\end_inset

.
 P, R and Fmeas denote the precision, recall and F-measure of detected vocal
 onsets.
 Results are averaged per usul.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "results_all"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Vocal onset detection
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Is portamento in Turkish singing another reason 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We measured vocal onset accuracy in terms of precision and recall
\begin_inset Foot
status open

\begin_layout Plain Layout
We used the evaluation script available at 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/craffel/mir_eval"
target "https://github.com/craffel/mir_eval"

\end_inset


\end_layout

\end_inset

.
 Unlike 
\emph on
a cappella
\emph default
 singing, the exact onset times of singing voice accompanied by instruments,
 might be much more ambiguous.
 To accommodate this fact, we adopted the tolerance of 
\begin_inset Formula $t=50\thinspace ms$
\end_inset

, used for vocal onsets in accompanied flamenco singing by 
\begin_inset CommandInset citation
LatexCommand cite
key "kroher2015automatic"

\end_inset

, which is much bigger than the 
\begin_inset Formula $t=5\thinspace ms$
\end_inset

 used by 
\begin_inset CommandInset citation
LatexCommand citet
key "mauch2015computer"

\end_inset

 for 
\emph on
a cappella
\emph default
.
 Note that measuring transcription accuracy remains outside the scope of
 this thesis.
\begin_inset Note Comment
status collapsed

\begin_layout Plain Layout
annotation of vocal parts results practically in zero onsets detected in
 non-vocal regions.
 That is why vocal onsets are evaluated in practice only in vocal time intervals.
 
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Experiment 1: With manually annotated beats
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
IMPL: 
\end_layout

\begin_layout Plain Layout
to allows changing trans probs only at beat positions, set 
\begin_inset CommandInset href
LatexCommand href
name "SMOOTHING_WINDOW =0 "
target "https://github.com/georgid/pypYIN/blob/master/pypYIN/MonoNoteParameters.py#L50 "

\end_inset


\end_layout

\end_inset

As a precursor to evaluating the full-fledged model, we conducted an experiment
 with manually annotated beats.
 This is done to test the general feasibility of the proposed note transition
 model (presented in 
\begin_inset CommandInset ref
LatexCommand ref
reference "par:Note-transition-probability"

\end_inset

), unbiased from errors in the beat detection.
\end_layout

\begin_layout Standard
We did apply both the simple and the time-redistribution weighting schemes
 for 
\begin_inset Formula $\Theta(\phi_{k})$
\end_inset

, presented respectively in Eq.
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:weight_detected"

\end_inset

 and in Eq.
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:weight_anno"

\end_inset

.
 In preliminary experiments we saw that with annotated beats the simple
 weighting results in much worse onset accuracy than the time-redistributed
 one.
 Therefore the experimental results reported are conducted with the latter
 weighting scheme.
 
\end_layout

\begin_layout Standard
We have tested different pairs of values for 
\begin_inset Formula $w$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

 from Eq.
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:note_onset_trans_prob"

\end_inset

.The onset detection accuracy peaked at 
\begin_inset Formula $w=1.2$
\end_inset

 and 
\begin_inset Formula $\sigma=30\thinspace ms$
\end_inset

.
 Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "results_all"

\end_inset

 presents the accuracies compared to the baseline.
 Inspection of detections showed that the proposed model added some onsets
 around beats, which are missed by the baseline.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
TODO box-plot chart with different values of weight w.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Experiment 2: Full model 
\end_layout

\begin_layout Standard
To assure computational speed, we did extend the efficient implementation
 of the joint bar-tempo state space and the Viterbi algorithm of the 
\emph on
madmom
\emph default
 toolbox
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
We did a fork of the madmom toolbox 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/CPJKU/madmom/"
target "https://github.com/CPJKU/madmom/"

\end_inset

, which we make available at 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/georgid/madmom"
target "https://github.com/georgid/madmom"

\end_inset


\end_layout

\end_inset

.
 The average F-measure of detected beats for the different metrical cycles
 can be seen in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "results_all"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
per-recoding results can be found in sheet 2 of 
\begin_inset CommandInset href
LatexCommand href
target "https://tinyurl.com/kz4mpkz"

\end_inset


\end_layout

\end_inset

.
 For aksak and d
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
"u}
\end_layout

\end_inset

yek usuls, the accuracy is somewhat worse than the results of 
\begin_inset Formula $91$
\end_inset

 and 
\begin_inset Formula $85.2$
\end_inset

 respectively, reported in 
\begin_inset CommandInset citation
LatexCommand cite
after "Table 1.a-c, R=1"
key "holzapfel2014tracking"

\end_inset

.
 We believe the reason is in the smaller size of our training data.
 Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "results_all"

\end_inset

 evidences also a reasonable improvement of the vocal onset detection accuracy
 for both music traditions.
 The results reported are only with the simple weighting scheme for the
 vocal note onset transition model (the time-redistribution weighting was
 not implemented in this experiment).
 
\end_layout

\begin_layout Standard
Adding the automatic beat tracking improved the baseline, whereas this was
 not the case with manual beats for simple weighting.
 This suggests that the concurrent tracking of beats and vocal onsets is
 a flexible strategy and can accommodate some vocal onsets, slightly time-shifte
d from a beat.
 We observe also that the vocal onset accuracy is on average a bit inferior
 to that with manual beat annotations (done with the time-redistribution
 weighting).
 
\color black
All the experiments in this Section are to be presented in 
\begin_inset CommandInset citation
LatexCommand citet
key "dzhambazov2017metrical"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
if not onsets are used, the beat detection is still the same, 
\end_layout

\begin_layout Plain Layout
IMPL: https://GitHub.com/Georgia/pippin/blob/master/pippin/MonoNoteParameters.py#L
68 WITH_NOTE_STATES=0
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
imply: /Users/joro/workspace/madmom_notes/run_beats
\end_layout

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
IMPL: https://GitHub.com/Georgia/pippin/blob/master/report/scripts/eval_beats
\end_layout

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
IMPL: https://GitHub.com/Georgia/pippin/blob/master/report/scripts/eval_onsets
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section

\color black
Onset-aware lyrics-to-audio alignment
\begin_inset CommandInset label
LatexCommand label
name "sec:Onset-aware-lyrics-to-audio-alig"

\end_inset


\end_layout

\begin_layout Standard
In the previous section 
\color black
we investigated the relation of metrical accents to the positions of vocal
 onsets.

\color inherit
 We proposed a method for automatic vocal onset detection in a way aware
 of metrical accents.
 Using as input the detected vocal onsets, in this chapter we propose a
 strategy to improve 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{LAA}
\end_layout

\end_inset

 by representing the interaction of vocal onsets with syllable transitions.
 In this way the influence of metrical events on syllable transitions is
 represented implicitly through its influence on vocal note onsets, which
 are in turn influenced by metrical events.

\color black
 
\end_layout

\begin_layout Standard
As we saw in the previous chapter, automatically determining the time positions
 of transitions between sung syllables can be greatly  assisted by information
 from the music score.
 Similarly, by relying on music score, one can infer automatically the timestamp
s of vocal note onsets.
 Such timestamps are estimated reasonably well by a recent study on automatic
 score-toâ€“audio alignment 
\begin_inset CommandInset citation
LatexCommand citet
after "chapter 6"
key "senturk2016thesis"

\end_inset

.
 In contrast, with the help of automatic singing voice transcription, vocal
 note onsets can be derived without the need for music score 
\begin_inset CommandInset citation
LatexCommand citep
key "benetos2013automatic"

\end_inset

.

\color black
 
\color inherit
Since we intend that the proposed methodologies can be applicable for material
 with no music scores available, we preferred to apply automatic vocal onset
 detection instead of score-to-audio alignment.
 Detecting vocal onsets in any setting is arguably one of the hardest 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{MIR}
\end_layout

\end_inset

 problems.
 Still, for the study of onset-aware phoneme transitions, it is important
 that onsets timestamps are detected as accurately as possible.
 To 
\color black
assure better accuracy
\color inherit
, 
\color black
experiments in this section are conducted only on 
\emph on
a cappella
\emph default
 material from 
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{OTMM}
\end_layout

\end_inset


\color black
, as well as with manually annotated onsets.
 
\end_layout

\begin_layout Standard

\color black
An overview of the proposed approach is presented in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "overview"

\end_inset

.
 
\color inherit
As in all approaches presented in this thesis, first an audio recording
 is manually divided into segments according to the coarse level complementary
 context â€” the sections of the composition.
 The boundaries of vocal section (one of 
\emph on
zemin
\emph default
, 
\emph on
nakarat
\emph default
, 
\emph on
meyan
\emph default
) are taken from manual annotations.
 An audio recording and its corresponding lyrics are input.

\color black
 The vocal note onsets (automatically detected or manually annotated) together
 with phoneme transition rules are fed as input to the transition model.
 The phonetic recognizer, guided by the phoneme transition rules, returns
 the start and end timestamps of the aligned phonemes.
 
\end_layout

\begin_layout Standard

\color black
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\color black
\begin_inset Graphics
	filename Phd_Figs/onsets/LAA_onsets_overview.png
	lyxscale 20
	width 100col%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Overview of the modules of the proposed approach.
 The transition model is derived from phoneme transition rules and onset
 positions from the singing voice transcription.
 Then it input to the phonetic recognizer, together with the phonemes network
 and the features, extracted from audio segments.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "overview"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection

\color black
Phoneme transition rules
\begin_inset CommandInset label
LatexCommand label
name "subsec:Phoneme-transition-rules"

\end_inset


\end_layout

\begin_layout Standard

\color black
The transition to a consecutive lyrics syllable implies a concurrent transition
 to a new note.
 The onset of the new note occurs usually at the start of the first voiced
 sound in the syllable.
 If we look at this reversely, the occurrence of note attack in a sung melody
 can signal a phonetic transition.
 The transition depends on the phoneme types, since, for example, a new
 note cannot start at unvoiced consonants.
 Taking advantage of that fact, we formulate rules that guide the transition
 between consecutive phonemes when a note onset is present.
 In general, we consider note onsets (attack) events as complementary context
 of phonetic timbre.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\color black
ADD.
 introduce SUndberg 2006 or remove.
 but they have not been used for tracking voice
\end_layout

\end_inset

Similar phoneme transitions rules have been used successfully to enhance
 the perceived naturalness of synthesized singing voice 
\begin_inset CommandInset citation
LatexCommand citep
key "sundberg2006kth"

\end_inset

.
 The onset-aware phoneme transitions rules, we designed, have been presented
 in 
\begin_inset CommandInset citation
LatexCommand citet
key "dzhambazov2016onsetLyrics_ismir"

\end_inset

.
\end_layout

\begin_layout Standard

\color black
We formalize transition rules described in this Section for Turkish, in
 which each syllable has exactly one vowel.
 In this sense, the rules could be transferred to another language with
 single-vowel syllables
\begin_inset Foot
status open

\begin_layout Plain Layout

\color black
Among single-vowel syllabic languages are also Japanese and to some extent
 Italian
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard

\color black
Let 
\begin_inset Formula $V$
\end_inset

 denote a vowel, 
\begin_inset Formula $C$
\end_inset

 denote a consonant and 
\begin_inset Formula $L$
\end_inset

 denote a vowel, liquid (LL, M, NN) or the semivowel Y.
 Rules 
\begin_inset Formula $R1$
\end_inset

 and 
\begin_inset Formula $R2$
\end_inset

 represent inter-syllable transition, e.g.
 phoneme 
\begin_inset Formula $i$
\end_inset

 is followed by phoneme 
\begin_inset Formula $j$
\end_inset

 from the following syllable:
\begin_inset Formula 
\begin{equation}
\begin{array}{c}
\thinspace\thinspace R1:\quad i=V\quad j=\neg L\\
R2:\quad i=C\quad j=L\thinspace\thinspace\ 
\end{array}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\color black
For example, for rule 
\begin_inset Formula $R2$
\end_inset

 if a syllable ends in a consonant, a note onset imposes with high probability
 that a transition to the following syllable is done, provided that it starts
 with a vowel.
 The same rule applies if it starts with a liquid, according to the observation
 that
\emph on
 
\emph default
pitch change takes place during a liquid preceding the vowel 
\begin_inset CommandInset citation
LatexCommand cite
after "timing of pitch change"
key "sundberg2006kth"

\end_inset

.
 Rule R2 is valid also for intra-syllabic phoneme patterns, together with
 rule R3:
\end_layout

\begin_layout Standard

\color black
\begin_inset Formula 
\begin{equation}
\begin{array}{c}
R3:\quad i=V\quad j=C\end{array}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\color black
Essentially, if the current phoneme is vocal and the next one is non-voiced
 (e.g.
 
\begin_inset Formula $R1$
\end_inset

, 
\begin_inset Formula $R3$
\end_inset

) the transition to the next is discouraged.
 An example of the intra-syllable 
\begin_inset Formula $R2$
\end_inset

 can be seen for the syllable KK-AA in Figure 
\color inherit
5.3
\color black
 where the note onset triggers the change to the vowel AA.
 Unlike that, an onset for example, to the syllable Y to onset at Y for
 the syllable Y-E-T.
 
\end_layout

\begin_layout Standard

\color black
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\color black
\begin_inset Graphics
	filename Phd_Figs/onsets/screen_shot_sonic_vis.png
	width 100text%
	height 20pheight%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Ground truth annotation of syllables (in orange/top), phonemes (in red/middle)
 and notes (with blue/changing position).
 Audio excerpt corresponding to the word ÅŸikayet with syllables SH-IY, KK-AA
 and Y-E-T.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "example-onset-annotation"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection

\color black
Transition model
\begin_inset CommandInset label
LatexCommand label
name "subsec:Transition-model"

\end_inset


\end_layout

\begin_layout Standard

\color black
The phoneme transitions are dependent on the current vocal note temporal
 segment.
 When a note is in its onset segment, the transition between phonemes could
 be conditioned differently, compared to when a note is in a another segment.
 
\color inherit
A crucial limitation of the phonetic recognizer 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{HMM}
\end_layout

\end_inset

 is the single latent variable, which can represent only one music facet
 - phonetic timbre.

\emph on
 
\emph default
To represent the influence of events of different music facets
\emph on
,
\emph default
\color black
 such as vocal note segments, one can use the hidden variables in a 
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{DBN}
\end_layout

\end_inset


\color black
 (see in Figure 
\color inherit
5.4).
\begin_inset Note Note
status open

\begin_layout Plain Layout
broken link to figure?
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\color black
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center

\color black
\begin_inset Graphics
	filename Phd_Figs/note_phoneme_DBN.jpeg
	lyxscale 20
	width 50page%

\end_inset


\end_layout

\begin_layout Plain Layout

\color black
\begin_inset Caption Standard

\begin_layout Plain Layout
A 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{DBN}
\end_layout

\end_inset

 for the simultaneous music note and phoneme states.
 Circles and squares denote continuous and discrete variables, respectively.
 Gray nodes and white nodes represent observed and hidden variables, respectivel
y.
 A phoneme transition is conditioned on the vocal note state.
 If a note onset is present the likelihood of transition is modified according
 to what the current 
\begin_inset Formula $h_{k-1}$
\end_inset

 and its following 
\begin_inset Formula $h_{k}$
\end_inset

 phoneme are.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\color black
\begin_inset CommandInset label
LatexCommand label
name "DBN-Onsets-Phonemes"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\color black
For particular states, transitions are modified depending on the presence
 of time-adjacent note onset.
 Let 
\begin_inset Formula $k'$
\end_inset

 be the timestamp of the onset 
\begin_inset Formula $\Delta n_{k'}=1$
\end_inset

, 
\begin_inset Note Note
status open

\begin_layout Plain Layout
 this is the first time the note activation function is used.
 introduce it!
\end_layout

\end_inset

which is closest to given time 
\begin_inset Formula $k$
\end_inset

.
 Now the transition probability can be rewritten as 
\end_layout

\begin_layout Standard

\color black
\begin_inset Formula 
\begin{equation}
a_{ij}(k)=\begin{cases}
a_{ij}-g(k,k')q, & R1\text{\mbox{\thinspace{or}\thinspace}}R3\\
a_{ij}+g(k,k')q, & R2
\end{cases}\label{eq:transition model}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\color black
\begin_inset Formula $R1$
\end_inset

 to 
\begin_inset Formula $R3$
\end_inset

 stand the phoneme transition rules, which are applied in the phonemes network
 by picking the states 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

 for two consecutive phonemes.
 The term 
\begin_inset Formula $q$
\end_inset

 is a constant whereas 
\begin_inset Formula $g(k,k')$
\end_inset

 is a weighting factor sampled from a normal distribution with its peak
 (mean) at 
\begin_inset Formula $k'$
\end_inset

: 
\end_layout

\begin_layout Standard

\color black
\begin_inset Formula 
\begin{equation}
g(k,k')=\begin{cases}
f(k;k',\sigma^{2})\sim\mathcal{N}(k',\sigma^{2}), & |k-k'|\le\sigma\\
0 & else
\end{cases}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\color black
Since singing voice onsets are regions in time, they span over multiple
 consecutive frames.
 To reflect that fact, 
\begin_inset Formula $g(k,k')$
\end_inset

 serves to smooth in time the influence of the discrete detected 
\begin_inset Formula $\Delta n_{k}$
\end_inset

, where 
\begin_inset Formula $\sigma$
\end_inset

 has been selected to be 
\begin_inset Formula $0.075$
\end_inset

 seconds.
 In this way an onset influences a region of 
\begin_inset Formula $0.15$
\end_inset

 seconds - a value we found empirically to be the optimal.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\color black
IMPL: ParametersAlgo.ONSET_SIGMA
\end_layout

\end_inset

 Furthermore, this allows to handle slight timestamp inaccuracies of the
 estimated note onsets.
 
\end_layout

\begin_layout Subsection
Inference
\end_layout

\begin_layout Standard

\color black
The most likely state sequence is found by means of a forced alignment Viterbi
 decoding.
 Similarly to the inference for metrical-accent-aware detection of vocal
 onsets (see Section 
\color inherit

\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:With-manually-annotated"

\end_inset


\color black
), we apply a VTHMM.
 For the sake of brevity we will refer to the onset-aware alignment model
 as VTHMM.
 The standard transition probabilities in the Viterbi maximization step
 in Eq.
\color inherit

\begin_inset CommandInset ref
LatexCommand ref
reference "eq:time-variable-viterbi-1"

\end_inset


\color black
 are substituted for the onset-aware transitions 
\begin_inset Formula $a_{ij}(k)$
\end_inset

 from Eq.
\color inherit

\begin_inset CommandInset ref
LatexCommand ref
reference "eq:transition model"

\end_inset

:
\color black
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
 To be able to control the influence of the onsets, we have introduced a
 weighting factor 
\begin_inset Formula $\gamma$
\end_inset

:
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\color black
\begin_inset Formula 
\begin{equation}
\delta_{k}(j)=\max_{i\in(j,\thinspace j-1)}\delta_{k-1}(i)\thinspace a_{ij}(k)\thinspace b_{j}(O_{k})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\color black
\begin_inset Note Note
status open

\begin_layout Plain Layout
The one from baseline chapter.
 Mention here note onsets
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
With automatically detected onsets
\end_layout

\begin_layout Standard

\color black
To obtain reliable estimate of singing note onsets, we adapt the automatic
 singing transcription method, developed for polyphonic flamenco recordings
 
\begin_inset CommandInset citation
LatexCommand cite
key "kroher2015automatic"

\end_inset

.
 It has been designed to handle singing with high degree of vocal pitch
 embellishments.
 We expect that this made it suitable for material from 
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{OTMM}
\end_layout

\end_inset


\color black
 singing, which has many embellishments, too
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
We preferred it, because preliminary experiments showed that with default
 parameters it outperforms the algorithm of 
\begin_inset CommandInset citation
LatexCommand citet
key "mauch2015computer"

\end_inset

 with default parameters, which we extended in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Beat-aware-note-onset"

\end_inset


\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
and accompaniment of string instruments - doe that matter in Nadine?
\end_layout

\end_inset

.
 We replace the predominant vocal extraction method with the 
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{OTMM}
\end_layout

\end_inset


\color black
-tailored pitch detection method of 
\begin_inset CommandInset citation
LatexCommand citet
key "atli2014audio"

\end_inset

, which we described in Section 
\color inherit

\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Singing-voice-melody"

\end_inset


\color black
.
 
\end_layout

\begin_layout Standard

\color black
The algorithm of 
\begin_inset CommandInset citation
LatexCommand cite
key "kroher2015automatic"

\end_inset

 considers two cases of onsets: interval onsets and steady pitch onsets.
 A Gaussian derivative filter detects interval onsets as long-term change
 of the pitch contour, whereas steady-pitch onsets are inferred from pitch
 discontinuities.
 Since phoneme transitions are modified only when onsets are present, we
 opt for increasing recall at the cost of losing precision.
 This is achieved by reducing the value of the parameter 
\begin_inset Formula $cF$
\end_inset

: the minimum output of the Gaussian filter.
 Since the algorithm cannot be integrated easily in an 
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{HMM}
\end_layout

\end_inset


\color black
, note onset segmentation is performed as a preprocessing step to the actual
 alignment.
 The extracted note onsets are converted, as in the case of manually annotated
 onsets, to a binary onset activation at each frame 
\begin_inset Formula $\Delta n_{t}=(0,1)$
\end_inset

.
 
\end_layout

\begin_layout Subsection

\color black
Experiments
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{LAA}
\end_layout

\end_inset

 is evaluated on the 6-recording subset of the 
\emph on
a cappella lyrics OTMM dataset
\emph default
 (see Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Multi-instrumental-vocal-onsets"

\end_inset

), for which vocal onsets have been annotated.
 
\emph on
A cappella
\emph default
 was preferred because of the very low vocal onset detection accuracy on
 instrumentally-accompanied singing.
 Experiments are executed with the MLP-DirectM (direct mapping to the 
\emph on
MLP-English
\emph default
 acoustic model from Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Direct-cross-language-mapping"

\end_inset

).
\end_layout

\begin_layout Subsubsection
Evaluation metrics
\end_layout

\begin_layout Standard
Alignment accuracy is measured as the percentage of duration of correctly
 aligned words from the total audio duration (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:percentage_alignment"

\end_inset

 for an example).
 
\color black
Unlike previous chapters, in this experiment we preferred to measure accuracy
 at the finer level of words, since looking at boundaries of phrases, we
 could potentially miss certain onset locations with improvement over the
 baseline.
 To this end, we annotated also word boundaries in the 6-recording subset.
\end_layout

\begin_layout Standard
We measured vocal onset accuracy in terms of recall.
 Similarly to the experiments on vocal onset detection from the previous
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Beat-aware-note-onset"

\end_inset

, we adopted the tolerance of 
\begin_inset Formula $t=50\thinspace ms$
\end_inset

.
\end_layout

\begin_layout Subsubsection
With manually annotated onsets
\end_layout

\begin_layout Standard

\color black
Unfortunately, as we saw in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Background-on-automatic"

\end_inset

 note onsets could not be estimated from polyphonic recordings with high
 accuracy.
 To assure reasonable accuracy, we utilized firstly manually annotated note
 onsets.
 This is done to test the general feasibility of the proposed model, unbiased
 from errors in the note segmentation algorithm, and to set a glass-ceiling
 alignment accuracy.
 
\begin_inset Note Comment
status collapsed

\begin_layout Plain Layout

\color black
can be found here: https://docs.google.com/spreadsheets/d/1C1gJt8j3Cm14LtIxS0R10xy
AVx9MZO5qsCKRKD14aNM/edit?usp=sharing
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard

\color black
As a baseline we conduct alignment with unaffected phoneme transition probabilit
ies, e.g.
 setting all 
\begin_inset Formula $\Delta n_{t}=0$
\end_inset

, which is equivalent to the baseline, presented in Section 
\color inherit

\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Evaluation"

\end_inset


\color black
.
 This resulted in average alignment accuracy of 
\begin_inset Formula $79.2$
\end_inset

 %.
 We have tested with different values of 
\begin_inset Formula $q$
\end_inset

 from Eq.
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:transition model"

\end_inset

 achieving best accuracy of 
\begin_inset Formula $82.5$
\end_inset

% at 
\begin_inset Formula $q=0.23$
\end_inset

, which is used on in the next experiment
\color inherit
, too
\begin_inset Foot
status open

\begin_layout Plain Layout
Per-recording results can be found at 
\begin_inset CommandInset href
LatexCommand href
name "https://tinyurl.com/ksqsqla"
target "https://tinyurl.com/ksqsqla"

\end_inset


\end_layout

\end_inset


\color black
.
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\color black
IMPL: ParametersAlgo.WITH_ORACLE_ONSETS =1
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
With automatically detected onsets
\end_layout

\begin_layout Standard

\color black
We measured the impact of the note segmentation approach of 
\begin_inset CommandInset citation
LatexCommand cite
key "kroher2015automatic"

\end_inset

 (introduced in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Background-on-automatic"

\end_inset

), varying onset detection recall by changing the minimum output of the
 Gaussian filter (controlled by the parameter 
\begin_inset Formula $cF$
\end_inset

).
 Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "results"

\end_inset

 summarizes the alignment accuracy with the VTHMM depending on recall.
 On 
\emph on
a cappella
\emph default
 best improvement over the baseline is achieved at recall of 
\begin_inset Formula $72.3$
\end_inset

% (at 
\begin_inset Formula $cF=3.5$
\end_inset

).
 This is though still much lower than the best recall of 81-84% achieved
 for flamenco 
\begin_inset CommandInset citation
LatexCommand cite
key "kroher2015automatic"

\end_inset

.
 Setting recall higher than that degraded performance probably because there
 are too many false alarms, resulting in forcing false transitions.
 
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\color black
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="6">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
\begin_inset Formula $cF$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
4.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
4.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
3.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
3.0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
OR
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
57.2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
59.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
66.8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
72.3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
73.2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
AA
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
78.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
79.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
81.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
81.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color black
81.2
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout

\color black
\begin_inset Caption Standard

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
VTHMM
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 performance, depending on the sensitivity parameter 
\emph on
cF.

\emph default
 Vocal onset recall (OR) and alignment accuracy (AA) are reported as a total
 for all the recordings.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
OR is INVENTED!
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "results"

\end_inset


\end_layout

\end_inset


\begin_inset Note Comment
status collapsed

\begin_layout Plain Layout

\color black
to eval accuracy uncomment line self.extractNoteOnsetsAndEval(currSectionLink)
 in align.LyricsAligner.LyricsAligner.alignRecording.
 use a list of files with ground truth given at https://docs.google.com/spreadshee
ts/d/1C1gJt8j3Cm14LtIxS0R10xyAVx9MZO5qsCKRKD14aNM/edit?usp=sharing 
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\color black
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "example alignment"

\end_inset

 allows a glance at results at the level of detected phonemes: the baseline
 
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{HMM}
\end_layout

\end_inset


\color black
 switches to the following phoneme after some amount of time, relatively
 similar for all phonemes.
 One reason for this might be that the waiting time in a state in HMMs with
 a fixed transition matrix cannot be too long 
\begin_inset CommandInset citation
LatexCommand cite
key "yu2010hidden"

\end_inset

.
 In contrast, for VTHMM the presence of note onsets at vowels activates
 rules 
\begin_inset Formula $R1$
\end_inset

 or 
\begin_inset Formula $R3$
\end_inset

, which allows waiting in the same state longer, as there are more onsets
 (for example AA from the word SH-IY-KK-AA-Y-E-T has five associated onsets).
 We chose to modify 
\begin_inset Formula $cF$
\end_inset

 because setting it to lower values increases the recall of the
\emph on
 interval onsets
\emph default
 only.
 Often in our dataset several consecutive notes with different pitch correspond
 to the same vowel.
 In fact, due to some characteristic for 
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{OTMM}
\end_layout

\end_inset


\color black
 descending/ascending melody progressions, a single syllable may happen
 to span many notes (up to 12 in our dataset) 
\begin_inset CommandInset citation
LatexCommand citep
key "ederer2011theory"

\end_inset

.
 However, for cases of vowels held long on same pitch, conceptually VTHMM
 is not capable of bringing any benefit.
 This is illustrated in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "example alignment"

\end_inset

 by the prematurely detected end boundary of E from the word SH-IY-KK-AA-Y-E-T.
 Although no separate experiment for each rule was made, inspection of particula
r cases revalued almost no contribution of 
\emph on
R2
\emph default
, supposedly due to the difficulty of detecting onsets are syllables starting
 with unvoiced consonants.
 
\end_layout

\begin_layout Standard

\color black
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
In addition to that, we examined alignment accuracy per recording (Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "per-recording alignment"

\end_inset

).
 It can be observed that VTHMM performs consistently better than the baseline
 HMM (with some exceptions of where accuracy is close)
\begin_inset Foot
status open

\begin_layout Plain Layout
Per-recording results can be found at 
\begin_inset CommandInset href
LatexCommand href
name "https://tinyurl.com/ksqsqla"
target "https://tinyurl.com/ksqsqla"

\end_inset

 
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Plain Layout
THIS IS INVENTED!
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\color black
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center

\color black
\begin_inset Graphics
	filename Phd_Figs/onsets/Kimseye_phonemeLevel_comparison_VTHMM.png
	lyxscale 30
	width 60page%

\end_inset


\end_layout

\begin_layout Plain Layout

\color black
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of boundaries of phonemes for the word 
\emph on
ÅŸikayet
\emph default
 (SH-IY-KK-AA-Y-E-T): 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
emph{on top}
\end_layout

\end_inset

: spectrum and pitch; 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
emph{then from top to bottom}
\end_layout

\end_inset

: ground truth boundaries, phonemes detected with the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{HMM}
\end_layout

\end_inset

, detected onsets, phonemes detected with VTHMM; (excerpt from the recording
 
\emph on
Kimseye etmem ÅŸikayet
\emph default
 (in original sung by Bekir Unluater).
 
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "example alignment"

\end_inset


\color inherit

\begin_inset Note Comment
status open

\begin_layout Plain Layout
../../workspace/AlignmentDuration/ISMIR_noteOnsets/Kimseye_phonemeLevel_comparison_V
THMM.png
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\color black
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Summary
\end_layout

\begin_layout Standard
In this chapter we assessed the contribution of explicitly representing
 metrical accents (fine-level complementary context) for improving the tracking
 of sung lyrics.

\color black
 We studied the relation of metrical accents to lyrics in two steps: how
 metrical accents interact with vocal onsets and how the latter, in turn,
 interact with phoneme transitions.
 In this way, the influence of metrical events on syllable transitions is
 represented implicitly through its influence on note onsets, which are
 in turn influenced by metrical events.
 
\color inherit
Therefore, we presented two separate probabilistic models for two separate
 tasks: metrical-accent-aware vocal onset detection and onset-aware lyrics-to-au
dio alignment.
 We carry out an evaluation on material from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{OTMM}
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Paragraph
Metrical-accent-aware vocal onset detection
\end_layout

\begin_layout Standard
We strived to improve the automatic vocal note onset detection by incorporating
 information about their position in a metrical cycle (i.e.
 metrical accents).
 
\color black
To this end we proposed a 
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{DBN}
\end_layout

\end_inset


\color black
 for the simultaneous tracking of metrical position and vocal onsets.
 The main contribution is that the approach integrates in one coherent model
 two existing state of the art probabilistic approaches for different tasks:
 beat tracking and singing voice transcription.

\color red
 
\color inherit
We carried out an evaluation on a multi-instrument dataset from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{OTMM}
\end_layout

\end_inset

 with two different usul (meter) types.
 Context knowledge about the usul is built within the transition model of
 the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{DBN}
\end_layout

\end_inset

.
 Results confirmed that the proposed model reasonably improves vocal note
 onset detection accuracy compared to a baseline model that does not take
 the metrical position into account.
 The F-measure rises from 31% to 36 % for the dÃ¼yek usul, which has better
 beat detection F-measure and from 38 to 40 % for aksak usul.
 
\end_layout

\begin_layout Standard
Detecting vocal onsets is polyphonic audio is arguably one of the hardest
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{MIR}
\end_layout

\end_inset

 problems.

\color black
 Although not the goal of this thesis, the presented 
\color inherit

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{DBN}
\end_layout

\end_inset


\color black
 can be used for full-fledged singing voice transcription.
 
\end_layout

\begin_layout Paragraph
Onset-aware lyrics-to-audio alignment.
 
\end_layout

\begin_layout Standard

\color black
We extended the phonetic recognizer approach
\series bold
 
\series default
\color inherit
by modeling the singing voice onsets, occurring simultaneously with phoneme
 transitions.
 We conceptualized onset-aware phoneme transition rules
\color black
 and proposed how to integrate them into the
\color inherit
 transition model of the phonetic recognizer.
 The method was tested on an 
\emph on
a cappella
\emph default
 OTMM dataset.
 The new model resulted in a slight improvement of from 79.2 % for baseline,
 unaware of singing voice onsets, to 81.7 %.
 In particular, the onsets due to rules discouraging premature transition,
 the states of sustained vowels were allowed to have longer durations.

\color black
 
\color inherit
This is, to our knowledge, the first attempt to model explicitly onsets
 from the vocal melody in the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{LAA}
\end_layout

\end_inset

 decoding process itself.
\end_layout

\end_body
\end_document
